{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9361f101-2173-485b-a270-56e9e828473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class AudioFeatureExtractor(ABC):\n",
    "    def __init__(self, n_mfcc=20, max_pad_length=1000):\n",
    "        self.sr = None\n",
    "        self.n_mfcc = n_mfcc\n",
    "    \n",
    "    def extract_mfcc(self, processed_audio):\n",
    "        window_size = int(0.5 * self.sr)\n",
    "        step_size = int(0.25 * self.sr)\n",
    "        mfcc_list = []\n",
    "    \n",
    "        for start in range(0, len(processed_audio) - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window = processed_audio[start:end]\n",
    "            \n",
    "            if len(window) != window_size:\n",
    "                continue\n",
    "    \n",
    "            mfcc = librosa.feature.melspectrogram(y=window, sr=self.sr)\n",
    "            mfcc_list.append(mfcc.mean(axis=1))\n",
    "    \n",
    "        return mfcc_list\n",
    "        \n",
    "    @abstractmethod\n",
    "    def process_audio(self, audio):\n",
    "        pass\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path)\n",
    "            self.sr = sr\n",
    "            processed_audio = self.process_audio(audio)\n",
    "            return self.extract_mfcc(processed_audio)\n",
    "        except Exception as e:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd58911-b8f4-48cc-8603-1c1844e7fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from init_config import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, feature_dim=129):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.sigmoid(self.linear1(torch.nn.functional.normalize(xx, p=1.0, dim=1)))\n",
    "\n",
    "\n",
    "def load_kirigami_model():\n",
    "    # Phoneme Model!\n",
    "    my_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_phoneme_filter_model.load_state_dict(\n",
    "    torch.load(lr_phoneme_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_phoneme_filter_model.eval()\n",
    "\n",
    "    return my_phoneme_filter_model\n",
    "\n",
    "\n",
    "def load_background_filter_model():\n",
    "    # Background Model!\n",
    "    my_background_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_background_filter_model.load_state_dict(torch.load(bg_lr_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_background_filter_model.eval()\n",
    "\n",
    "    return my_background_filter_model\n",
    "\n",
    "\n",
    "def kirigami_filter_torch(s_full, threshold=0.5):\n",
    "    lr_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    # load the model if in kirigami_filters directory\n",
    "    if os.path.exists(\"./kirigami_filters/scipy_phoneme_filter.ckpt\"):\n",
    "        lr_phoneme_filter_model.load_state_dict(\n",
    "            torch.load(\"./kirigami_filters/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Phoneme filter model not found\")\n",
    "\n",
    "    lr_phoneme_filter_model.eval()\n",
    "    pred = (lr_phoneme_filter_model.forward(torch.Tensor(s_full)) >= threshold).long().numpy()\n",
    "    masked = (1 - pred) * s_full\n",
    "    return masked\n",
    "\n",
    "\n",
    "def kirigami_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "\n",
    "        sum = np.sum(fft)\n",
    "\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def kirigami_filter_reverse_fft(stft, stft_original):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            # output_sp[i] = stft[i]\n",
    "            output_sp[i] = stft_original[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def background_detection_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight_background)):\n",
    "            product = product + vv * weight_background[iw]\n",
    "        product = product // sum\n",
    "        product = product + bias_background\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"Background probability: \", i, z)\n",
    "        if z < BACKGROUND_LR_THRESHOLD:  # lower than threshold not background.\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Kirigami models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_kirigami_model():\n",
    "    \"\"\"Load the phoneme filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_background_filter_model():\n",
    "    \"\"\"Load the background noise filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/noisy_background_scipy_detector.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "phoneme_filter_model = load_kirigami_model()\n",
    "background_filter_model = load_background_filter_model()\n",
    "\n",
    "# Extract model weights\n",
    "weight_phoneme = phoneme_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_phoneme = phoneme_filter_model.linear1.bias.data[0].numpy()\n",
    "weight_background = background_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_background = background_filter_model.linear1.bias.data[0].numpy()\n",
    "\n",
    "\n",
    "class KirigamiExtractor(AudioFeatureExtractor):\n",
    "    def apply_kirigami_filter(self, stft, weight, bias, threshold=0.3):\n",
    "        \"\"\"Apply Kirigami logistic regression filter on STFT features (ensuring 129 features per frame).\"\"\"\n",
    "        output_stft = np.zeros_like(stft)\n",
    "\n",
    "        for i, frame in enumerate(stft):\n",
    "            frame = frame[:129]  # Ensure exactly 129 dimensions\n",
    "\n",
    "            sum_val = np.sum(frame) + 1e-6  # Avoid division by zero\n",
    "            product = np.dot(frame, weight) / sum_val + bias\n",
    "            prob = 1 / (1 + np.exp(-product))  # Sigmoid activation\n",
    "\n",
    "            if prob < threshold:  # If probability is low, keep frame\n",
    "                output_stft[i, :129] = frame  # Apply only to the valid region\n",
    "\n",
    "        return output_stft\n",
    "\n",
    "    # Process Audio File\n",
    "    def process_audio(self, audio, sr=16000, threshold=0.5):\n",
    "        \"\"\"Process an audio file through Kirigami models and save the filtered output.\"\"\"\n",
    "        # Load audio\n",
    "        # audio, sr = librosa.load(input_audio_path, sr=16000)\n",
    "\n",
    "        # Compute STFT (Ensure output has 129 feature bins)\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=256, hop_length=128))[:129, :].T  # Transpose for correct shape\n",
    "\n",
    "        # Apply Kirigami phoneme & background filters\n",
    "        filtered_stft_phoneme = self.apply_kirigami_filter(stft, weight_phoneme, bias_phoneme, threshold)\n",
    "        # filtered_stft_background = self.apply_kirigami_filter(filtered_stft_phoneme, weight_background, bias_background, threshold)\n",
    "\n",
    "        # Convert back to audio using inverse STFT\n",
    "        filtered_audio = librosa.istft(filtered_stft_phoneme.T, hop_length=128)\n",
    "\n",
    "        return filtered_audio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ce81e8-41a1-4780-8af2-0bc21e1e3803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with kirigami extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:15<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for kirigami - Gender: 0.9129\n",
      "\n",
      "Classification Report for kirigami - Gender:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        male       0.85      0.91      0.88     10669\n",
      "      female       0.91      0.85      0.88     10492\n",
      "       other       0.99      0.97      0.98     10735\n",
      "\n",
      "    accuracy                           0.91     31896\n",
      "   macro avg       0.92      0.91      0.91     31896\n",
      "weighted avg       0.92      0.91      0.91     31896\n",
      "\n",
      "\n",
      "Accuracy for kirigami - Accent: 0.9267\n",
      "\n",
      "Classification Report for kirigami - Accent:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          us       0.93      0.90      0.91      6940\n",
      "     ireland       0.91      0.99      0.95      6801\n",
      "   australia       0.94      0.89      0.91      6950\n",
      "\n",
      "    accuracy                           0.93     20691\n",
      "   macro avg       0.93      0.93      0.93     20691\n",
      "weighted avg       0.93      0.93      0.93     20691\n",
      "\n",
      "\n",
      "Accuracy for kirigami - Age: 0.8450\n",
      "\n",
      "Classification Report for kirigami - Age:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       teens       0.84      0.77      0.80      2357\n",
      "    fourties       0.84      0.93      0.89      2396\n",
      "   seventies       0.85      0.83      0.84      2446\n",
      "\n",
      "    accuracy                           0.84      7199\n",
      "   macro avg       0.84      0.84      0.84      7199\n",
      "weighted avg       0.84      0.84      0.84      7199\n",
      "\n",
      "\n",
      "=== ACCURACY SUMMARY ===\n",
      "\n",
      "Kirigami:\n",
      "  Gender: 91.29%\n",
      "  Accent: 92.67%\n",
      "  Age: 84.50%\n",
      "  Average: 89.48%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "\n",
    "class RawAudioExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        return audio\n",
    "\n",
    "class SyntheticSensorExtractor(AudioFeatureExtractor):\n",
    "    def __init__(self, window_length=256, hop_length=128, samples_per_window=10):\n",
    "        super().__init__()\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "        self.samples_per_window = samples_per_window\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        frames = [\n",
    "            audio[i:i + self.window_length]\n",
    "            for i in range(0, len(audio) - self.window_length + 1, self.hop_length)\n",
    "        ]\n",
    "        \n",
    "        processed_audio = np.zeros(len(audio))\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            fft = np.fft.fft(frame, n=self.window_length)\n",
    "            bins = np.array_split(fft, self.samples_per_window)\n",
    "            reduced_fft = np.array([np.mean(bin) for bin in bins])\n",
    "            \n",
    "            reconstructed_fft = np.zeros_like(fft, dtype=np.complex_)\n",
    "            step = len(fft) // len(reduced_fft)\n",
    "            \n",
    "            for j, val in enumerate(reduced_fft):\n",
    "                reconstructed_fft[j * step:(j + 1) * step] = val\n",
    "            \n",
    "            processed_frame = np.fft.ifft(reconstructed_fft).real\n",
    "            start = i * self.hop_length\n",
    "            end = min(start + self.window_length, len(processed_audio))\n",
    "            processed_audio[start:end] += processed_frame[:end - start]\n",
    "        \n",
    "        if np.max(np.abs(processed_audio)) > 0:\n",
    "            processed_audio = processed_audio / np.max(np.abs(processed_audio))\n",
    "            \n",
    "        return processed_audio\n",
    "\n",
    "class PrivacyMicExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        S = librosa.stft(audio, n_fft=256, hop_length=128)\n",
    "        frequencies = librosa.fft_frequencies(sr=self.sr, n_fft=256)\n",
    "        S_filtered = np.where(frequencies[:, None] <= 300, S, 0)\n",
    "        return librosa.istft(S_filtered, hop_length=128)\n",
    "\n",
    "class CoughSenseExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        n_fft = int(0.15 * self.sr)\n",
    "        if n_fft % 2 != 0:\n",
    "            n_fft += 1\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        S = np.abs(librosa.stft(audio.astype(np.float64), n_fft=n_fft, hop_length=hop_length, window='hamming'))\n",
    "        S_flattened = S.T\n",
    "        \n",
    "        pca = PCA(n_components=10)\n",
    "        S_reduced = pca.fit_transform(S_flattened)\n",
    "        S_reconstructed = pca.inverse_transform(S_reduced).T\n",
    "\n",
    "        filtered_audio = librosa.istft(S_reconstructed, \n",
    "                                     hop_length=hop_length,\n",
    "                                     win_length=n_fft,\n",
    "                                     window='hamming')\n",
    "        return filtered_audio\n",
    "\n",
    "class SamosaExtractor(AudioFeatureExtractor):\n",
    "    def __init__(self, window_length=0.6, hop_length=0.03):\n",
    "        super().__init__()\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        subsampled_audio = signal.resample(audio, int(len(audio) * 1000 / self.sr))\n",
    "        upsampled_audio = signal.resample(subsampled_audio, len(audio))\n",
    "        return upsampled_audio\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, feature_extractor: AudioFeatureExtractor, ds_path: str, extractor_name: str):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.ds_path = ds_path\n",
    "        self.extractor_name = extractor_name\n",
    "        self.features_path = \"extracted_features\"\n",
    "        os.makedirs(self.features_path, exist_ok=True)\n",
    "\n",
    "    def create_features_df(self, data):\n",
    "        features_filepath = os.path.join(self.features_path, f\"{self.extractor_name}_features30000.csv\")\n",
    "        features_list = []\n",
    "        \n",
    "        for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "            features = self.feature_extractor.extract_features(\n",
    "                os.path.join(self.ds_path, row['filename'])\n",
    "            )\n",
    "           \n",
    "            if features is not None:\n",
    "                for i in features:\n",
    "                    features_list.append([row['gender'], row['accent']] + i.tolist() + [row['age']])\n",
    "\n",
    "        feature_columns = [f\"feature_{i}\" for i in range(len(features_list[0])-3)]\n",
    "        features_df = pd.DataFrame(features_list, columns=['gender', 'accent'] + feature_columns + ['age'])\n",
    "        features_df.to_csv(features_filepath, index=False)\n",
    "        return features_df\n",
    "\n",
    "    def run_classification(self, features_df, target_col, classes, title):\n",
    "        \n",
    "        df_filtered = features_df[features_df[target_col].isin(classes)]\n",
    "        \n",
    "        # Drop rows where all feature values are zero\n",
    "        df_filtered = df_filtered.loc[~(df_filtered.drop(['gender', 'accent', 'age', target_col], axis=1) == 0).all(axis=1)]\n",
    "        \n",
    "        X = df_filtered.drop(['gender', 'accent', 'age'], axis=1)\n",
    "        y = df_filtered[target_col]\n",
    "        \n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        print(f\"\\nAccuracy for {title}: {accuracy:.4f}\")\n",
    "        print(f\"\\nClassification Report for {title}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=classes))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "\n",
    "def print_accuracy_summary(accuracies):\n",
    "    print(\"\\n=== ACCURACY SUMMARY ===\")\n",
    "    \n",
    "    for extractor, scores in accuracies.items():\n",
    "        print(f\"\\n{extractor.capitalize()}:\")\n",
    "        print(f\"  Gender: {scores['gender']:.2%}\")\n",
    "        print(f\"  Accent: {scores['accent']:.2%}\")\n",
    "        print(f\"  Age: {scores['age']:.2%}\")\n",
    "        avg = sum(scores.values()) / len(scores)\n",
    "        print(f\"  Average: {avg:.2%}\")\n",
    "\n",
    "def main():\n",
    "    ds_path = \"/work/pi_shenoy_umass_edu/sgomasta_umass_edu/data/Common Voice/cv-valid-train-main/cv-valid-train\"\n",
    "    metadata_path = \"/work/pi_shenoy_umass_edu/sgomasta_umass_edu/data/Common Voice/cv-valid-train.csv\"\n",
    "    \n",
    "    extractors = { \"kirigami\":KirigamiExtractor()\n",
    "        'coughsense': CoughSenseExtractor(),\n",
    "        'raw': RawAudioExtractor(),\n",
    "        'synthetic': SyntheticSensorExtractor(),\n",
    "        'privacy': PrivacyMicExtractor(),\n",
    "        'samosa': SamosaExtractor()\n",
    "    }\n",
    "\n",
    "    data = pd.read_csv(metadata_path)\n",
    "    data = data[\n",
    "        (data['gender'].isin(['male', 'female', 'other'])) |\n",
    "        (data['age'].isin(['teens', 'seventies', 'fourties'])) |\n",
    "        (data['accent'].isin(['us', 'ireland', 'australia']))\n",
    "    ]\n",
    "    \n",
    "    data = data[['filename', 'age', 'gender', 'accent']].dropna().head(5000)\n",
    "    data['filename'] = data['filename'].apply(os.path.basename)\n",
    "\n",
    "    accuracies = {}\n",
    "    for name, extractor in extractors.items():\n",
    "        print(f\"\\nProcessing with {name} extractor...\")\n",
    "        classifier = AudioClassifier(extractor, ds_path, name)\n",
    "        features_df = classifier.create_features_df(data)\n",
    "        \n",
    "        accuracies[name] = {\n",
    "            'gender': classifier.run_classification(\n",
    "                features_df, 'gender', ['male', 'female', 'other'], \n",
    "                f\"{name} - Gender\"\n",
    "            ),\n",
    "            'accent': classifier.run_classification(\n",
    "                features_df, 'accent', ['us', 'ireland', 'australia'], \n",
    "                f\"{name} - Accent\"\n",
    "            ),\n",
    "            'age': classifier.run_classification(\n",
    "                features_df, 'age', ['teens', 'fourties', 'seventies'], \n",
    "                f\"{name} - Age\"\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    print_accuracy_summary(accuracies)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f45943-09e6-4700-b1ae-491747aec62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-2022.10]",
   "language": "python",
   "name": "conda-env-anaconda-2022.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
