{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bec3e4-2bf3-4816-a428-82e8fb1b92cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename         category\n",
      "0   1-100032-A-0.wav              dog\n",
      "1  1-100038-A-14.wav   chirping_birds\n",
      "2  1-100210-A-36.wav   vacuum_cleaner\n",
      "3  1-100210-B-36.wav   vacuum_cleaner\n",
      "4  1-101296-A-19.wav     thunderstorm\n",
      "5  1-101296-B-19.wav     thunderstorm\n",
      "6  1-101336-A-30.wav  door_wood_knock\n",
      "7  1-101404-A-34.wav      can_opening\n",
      "8   1-103298-A-9.wav             crow\n",
      "9  1-103995-A-30.wav  door_wood_knock\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file in the ESC-50 dataset\n",
    "Path = '/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/ESC-50- UTILITY/esc-50/ESC-50-master/'\n",
    "csv_path = Path + 'meta/esc50.csv'\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "audio_path_prefix = Path+'/audio/'\n",
    "\n",
    "\n",
    "df = df[['filename', 'category']]\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d2749-289c-4f85-bdc6-b9150e6f791e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f11c50-151e-4d1f-9e8a-3467d7909c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cd9258-a925-4669-b9ef-96409d5e4b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dog', 'chirping_birds', 'vacuum_cleaner', 'thunderstorm',\n",
       "       'door_wood_knock', 'can_opening', 'crow', 'clapping', 'fireworks',\n",
       "       'chainsaw', 'airplane', 'mouse_click', 'pouring_water', 'train',\n",
       "       'sheep', 'water_drops', 'church_bells', 'clock_alarm',\n",
       "       'keyboard_typing', 'wind', 'footsteps', 'frog', 'cow',\n",
       "       'brushing_teeth', 'car_horn', 'crackling_fire', 'helicopter',\n",
       "       'drinking_sipping', 'rain', 'insects', 'laughing', 'hen', 'engine',\n",
       "       'breathing', 'crying_baby', 'hand_saw', 'coughing',\n",
       "       'glass_breaking', 'snoring', 'toilet_flush', 'pig',\n",
       "       'washing_machine', 'clock_tick', 'sneezing', 'rooster',\n",
       "       'sea_waves', 'siren', 'cat', 'door_wood_creaks', 'crickets'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df['category'].unique()\n",
    "# df['target'].unique()\n",
    "# df[['target', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af8dbab-a24b-444d-be63-d0c54294dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset metadata\n",
    "csv_path = Path+'/meta/esc50.csv'\n",
    "df = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430cf2c2-17df-43c7-9021-f65879061468",
   "metadata": {},
   "source": [
    "<h1> Flatten Mel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c43af70-d377-40ef-98b4-117fef3af487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Processing Classes\n",
    "class AudioFeatureExtractor:\n",
    "    def process_audio(self, audio, sr):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RawAudioExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio, sr):\n",
    "        return audio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39588d2a-1e17-4c9c-a9fe-e9b9599d8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from init_config import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, feature_dim=129):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.sigmoid(self.linear1(torch.nn.functional.normalize(xx, p=1.0, dim=1)))\n",
    "\n",
    "\n",
    "def load_kirigami_model():\n",
    "    # Phoneme Model!\n",
    "    my_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_phoneme_filter_model.load_state_dict(\n",
    "    torch.load(lr_phoneme_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_phoneme_filter_model.eval()\n",
    "\n",
    "    return my_phoneme_filter_model\n",
    "\n",
    "\n",
    "def load_background_filter_model():\n",
    "    # Background Model!\n",
    "    my_background_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_background_filter_model.load_state_dict(torch.load(bg_lr_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_background_filter_model.eval()\n",
    "\n",
    "    return my_background_filter_model\n",
    "\n",
    "\n",
    "def kirigami_filter_torch(s_full, threshold=0.5):\n",
    "    lr_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    # load the model if in kirigami_filters directory\n",
    "    if os.path.exists(\"./kirigami_filters/scipy_phoneme_filter.ckpt\"):\n",
    "        lr_phoneme_filter_model.load_state_dict(\n",
    "            torch.load(\"./kirigami_filters/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Phoneme filter model not found\")\n",
    "\n",
    "    lr_phoneme_filter_model.eval()\n",
    "    pred = (lr_phoneme_filter_model.forward(torch.Tensor(s_full)) >= threshold).long().numpy()\n",
    "    masked = (1 - pred) * s_full\n",
    "    return masked\n",
    "\n",
    "\n",
    "def kirigami_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "\n",
    "        sum = np.sum(fft)\n",
    "\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def kirigami_filter_reverse_fft(stft, stft_original):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            # output_sp[i] = stft[i]\n",
    "            output_sp[i] = stft_original[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def background_detection_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight_background)):\n",
    "            product = product + vv * weight_background[iw]\n",
    "        product = product // sum\n",
    "        product = product + bias_background\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"Background probability: \", i, z)\n",
    "        if z < BACKGROUND_LR_THRESHOLD:  # lower than threshold not background.\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Kirigami models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_kirigami_model():\n",
    "    \"\"\"Load the phoneme filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_background_filter_model():\n",
    "    \"\"\"Load the background noise filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/noisy_background_scipy_detector.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "phoneme_filter_model = load_kirigami_model()\n",
    "background_filter_model = load_background_filter_model()\n",
    "\n",
    "# Extract model weights\n",
    "weight_phoneme = phoneme_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_phoneme = phoneme_filter_model.linear1.bias.data[0].numpy()\n",
    "weight_background = background_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_background = background_filter_model.linear1.bias.data[0].numpy()\n",
    "\n",
    "\n",
    "class KirigamiExtractor(AudioFeatureExtractor):\n",
    "    def apply_kirigami_filter(self, stft, weight, bias, threshold=0.5):\n",
    "        \"\"\"Apply Kirigami logistic regression filter on STFT features (ensuring 129 features per frame).\"\"\"\n",
    "        output_stft = np.zeros_like(stft)\n",
    "\n",
    "        for i, frame in enumerate(stft):\n",
    "            frame = frame[:129]  # Ensure exactly 129 dimensions\n",
    "\n",
    "            sum_val = np.sum(frame) + 1e-6  # Avoid division by zero\n",
    "            product = np.dot(frame, weight) / sum_val + bias\n",
    "            prob = 1 / (1 + np.exp(-product))  # Sigmoid activation\n",
    "\n",
    "            if prob < threshold:  # If probability is low, keep frame\n",
    "                output_stft[i, :129] = frame  # Apply only to the valid region\n",
    "\n",
    "        return output_stft\n",
    "\n",
    "    # Process Audio File\n",
    "    def process_audio(self, audio, sr=16000, threshold=0.5):\n",
    "        \"\"\"Process an audio file through Kirigami models and save the filtered output.\"\"\"\n",
    "        # Load audio\n",
    "        # audio, sr = librosa.load(input_audio_path, sr=16000)\n",
    "\n",
    "        # Compute STFT (Ensure output has 129 feature bins)\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=256, hop_length=128))[:129, :].T  # Transpose for correct shape\n",
    "\n",
    "        # Apply Kirigami phoneme & background filters\n",
    "        filtered_stft_phoneme = self.apply_kirigami_filter(stft, weight_phoneme, bias_phoneme, threshold)\n",
    "        # filtered_stft_background = self.apply_kirigami_filter(filtered_stft_phoneme, weight_background, bias_background, threshold)\n",
    "\n",
    "        # Convert back to audio using inverse STFT\n",
    "        filtered_audio = librosa.istft(filtered_stft_phoneme.T, hop_length=128)\n",
    "\n",
    "        return filtered_audio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6f1eb0-66c2-4b2d-ab7a-b20f9e7966c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing kirigami filtering...\n",
      "Feature extraction completed for kirigami. Saved to flattened_mel_spectrogram_kirigami.csv\n",
      "All feature extraction processes are complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the correct absolute path\n",
    "base_path = \"/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/ESC-50- UTILITY/esc-50/ESC-50-master\"\n",
    "\n",
    "# Load the dataset metadata\n",
    "csv_path = os.path.join(base_path, \"meta\", \"esc50.csv\")\n",
    "\n",
    "# Ensure dataset exists\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"Error: The file '{csv_path}' does not exist. Please check the path.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Define the audio file path\n",
    "audio_path_prefix = os.path.join(base_path, \"audio\")\n",
    "\n",
    "# Ensure the audio directory exists\n",
    "if not os.path.exists(audio_path_prefix):\n",
    "    raise FileNotFoundError(f\"Error: The audio directory '{audio_path_prefix}' does not exist.\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# class KirigamiExtractor(AudioFeatureExtractor):\n",
    "#     def __init__(self, model_path=\"/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/kirigami3.pkl\", sr=16000, n_fft=256, hop_length=128, window_size=1.0, stride=0.5):\n",
    "#         \"\"\"Initialize Kirigami for filtering after loading the trained model.\"\"\"\n",
    "#         self.sr = sr\n",
    "#         self.n_fft = n_fft\n",
    "#         self.hop_length = hop_length\n",
    "#         self.window_size = int(window_size * sr)\n",
    "#         self.stride = int(stride * sr)\n",
    "#         self.model_path = model_path\n",
    "#         with open(self.model_path, \"rb\") as f:\n",
    "#                 self.model = pickle.load(f)\n",
    "#     def process_audio(self, audio, sr):\n",
    "#         \"\"\"Apply the speech filter window-by-window.\"\"\"\n",
    "#         num_samples = len(audio)\n",
    "#         filtered_audio = np.zeros_like(audio)\n",
    "\n",
    "#         for start in range(0, num_samples - self.window_size, self.stride):\n",
    "#             window = audio[start : start + self.window_size]\n",
    "#             stft = np.abs(librosa.stft(window, n_fft=self.n_fft, hop_length=self.hop_length))\n",
    "#             norm = np.linalg.norm(stft, ord=1, axis=0, keepdims=True)\n",
    "#             norm[norm == 0] = 1\n",
    "#             stft = stft / norm\n",
    "#             features = np.nan_to_num(stft.T)\n",
    "#             prediction = self.model.predict(features.reshape(1, -1))\n",
    "\n",
    "#             if prediction < 0.5:\n",
    "#                 filtered_audio[start : start + self.window_size] = window  # Keep non-speech\n",
    "\n",
    "#         return filtered_audio\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class SyntheticSensorExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio, sr):\n",
    "        return signal.resample(audio, len(audio) // 2)\n",
    "\n",
    "class PrivacyMicExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio, sr):\n",
    "        S = librosa.stft(audio, n_fft=256, hop_length=128)\n",
    "        frequencies = librosa.fft_frequencies(sr=sr, n_fft=256)\n",
    "        S_filtered = np.where(frequencies[:, None] <= 300, S, 0)\n",
    "        return librosa.istft(S_filtered, hop_length=128)\n",
    "\n",
    "class CoughSenseExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio, sr):\n",
    "        n_fft = int(0.15 * sr)\n",
    "        hop_length = n_fft // 2\n",
    "        S = np.abs(librosa.stft(audio.astype(np.float64), n_fft=n_fft, hop_length=hop_length))\n",
    "        pca = PCA(n_components=10)\n",
    "        S_reduced = pca.fit_transform(S.T)\n",
    "        S_reconstructed = pca.inverse_transform(S_reduced).T\n",
    "        return librosa.istft(S_reconstructed, hop_length=hop_length)\n",
    "\n",
    "class SamosaExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio, sr):\n",
    "        subsampled_audio = signal.resample(audio, len(audio) // 2)\n",
    "        return signal.resample(subsampled_audio, len(audio))\n",
    "\n",
    "# Feature Extraction Function (Flattened Mel-Spectrogram)\n",
    "def extract_melspectrogram(audio, sr, label, window_size=0.5, hop_size=0.3):\n",
    "    \"\"\"Extracts Mel-Spectrogram features using a sliding window approach, limited to 10 windows per file.\"\"\"\n",
    "    window_length = int(window_size * sr)\n",
    "    hop_length = int(hop_size * sr)\n",
    "\n",
    "    all_features = []\n",
    "    window_count = 0\n",
    "\n",
    "    for start in range(0, len(audio) - window_length + 1, hop_length):\n",
    "\n",
    "        window_segment = audio[start:start + window_length]\n",
    "        # if len(window_segment) != window_length:\n",
    "        #     continue  \n",
    "\n",
    "        # Compute Mel-Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=window_segment, sr=sr)\n",
    "\n",
    "        # Flatten the spectrogram to a 1D feature vector\n",
    "        mel_spectrogram_flattened = mel_spectrogram.mean(axis=1)\n",
    "\n",
    "        # Create dictionary of features\n",
    "        features = {f\"mel_{i}\": mel_spectrogram_flattened[i] for i in range(len(mel_spectrogram_flattened))}\n",
    "        features['Label'] = label\n",
    "        features['Window Index'] = window_count  # Track window index\n",
    "\n",
    "        all_features.append(features)\n",
    "        window_count += 1  \n",
    "\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "# Initialize Extractors\n",
    "extractors = {  \"kirigami\":KirigamiExtractor(),\n",
    "    #\"RawAudio\": RawAudioExtractor(),\n",
    "    #\"SyntheticSensor\": SyntheticSensorExtractor(),\n",
    "    #\"PrivacyMic\": PrivacyMicExtractor(),\n",
    "   # \"CoughSense\": CoughSenseExtractor(),\n",
    "    #\"Samosa\": SamosaExtractor()\n",
    "    # \"kirigami\":KirigamiExtractor()\n",
    "}\n",
    "\n",
    "# Extract features for each filtering technique\n",
    "for extractor_name, extractor in extractors.items():\n",
    "    print(f\"Processing {extractor_name} filtering...\")\n",
    "\n",
    "    features_list = pd.DataFrame()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        file_path = os.path.join(audio_path_prefix, row['filename'])\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: Skipping missing file {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load raw audio\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Process with the chosen filtering method\n",
    "        processed_audio = extractor.process_audio(y, sr)\n",
    "\n",
    "        # Extract features\n",
    "        features = extract_melspectrogram(processed_audio, sr, row['target'])\n",
    "        features_list = features_list._append(features, ignore_index=True)\n",
    "\n",
    "    # Save the extracted features\n",
    "    output_file = f\"flattened_mel_spectrogram_{extractor_name}.csv\"\n",
    "    features_list.to_csv(output_file, index=False)\n",
    "    print(f\"Feature extraction completed for {extractor_name}. Saved to {output_file}\")\n",
    "\n",
    "print(\"All feature extraction processes are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4787dd8-f171-454a-b053-445a8dd5d84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af8272c-b983-4a25-b0d8-262d57d55036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Processing dataset: flattened_mel_spectrogram_RawAudio.csv\n",
      "âœ… Model trained on flattened_mel_spectrogram_RawAudio.csv features.\n",
      "ðŸŽ¯ Accuracy: 0.8420\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7830    0.7477    0.7650       111\n",
      "           1     0.7654    0.6966    0.7294        89\n",
      "           2     0.8333    0.7895    0.8108       114\n",
      "           3     0.9266    0.8783    0.9018       115\n",
      "           4     0.9248    0.9609    0.9425       128\n",
      "           5     0.7475    0.6789    0.7115       109\n",
      "           6     0.8106    0.8843    0.8458       121\n",
      "           7     0.8429    0.9593    0.8973       123\n",
      "           8     0.8696    0.9449    0.9057       127\n",
      "           9     0.9344    0.9661    0.9500       118\n",
      "          10     0.9014    1.0000    0.9481       128\n",
      "          11     0.8176    0.9453    0.8768       128\n",
      "          12     0.8741    0.9440    0.9077       125\n",
      "          13     0.9624    1.0000    0.9808       128\n",
      "          14     0.9000    0.9844    0.9403       128\n",
      "          15     0.8131    0.8056    0.8093       108\n",
      "          16     0.8411    0.9922    0.9104       128\n",
      "          17     0.8175    0.8583    0.8374       120\n",
      "          18     0.7895    0.8268    0.8077       127\n",
      "          19     0.7552    0.8438    0.7970       128\n",
      "          20     0.8281    0.8413    0.8346       126\n",
      "          21     0.4342    0.3976    0.4151        83\n",
      "          22     0.8345    0.9355    0.8821       124\n",
      "          23     0.8163    0.7477    0.7805       107\n",
      "          24     0.8421    0.3902    0.5333        82\n",
      "          25     0.7333    0.8115    0.7704       122\n",
      "          26     0.8750    0.6087    0.7179       115\n",
      "          27     0.7778    0.9032    0.8358       124\n",
      "          28     0.6802    0.9213    0.7826       127\n",
      "          29     0.6571    0.4381    0.5257       105\n",
      "          30     0.6061    0.4762    0.5333        84\n",
      "          31     0.8208    0.8056    0.8131       108\n",
      "          32     0.8000    0.8254    0.8125       126\n",
      "          33     0.8750    0.6087    0.7179       115\n",
      "          34     0.7195    0.6556    0.6860        90\n",
      "          35     0.9685    0.9685    0.9685       127\n",
      "          36     0.9504    0.8984    0.9237       128\n",
      "          37     0.9831    0.9587    0.9707       121\n",
      "          38     0.9225    0.9297    0.9261       128\n",
      "          39     0.8750    0.3231    0.4719        65\n",
      "          40     0.9583    0.9200    0.9388       125\n",
      "          41     0.8926    0.8438    0.8675       128\n",
      "          42     0.9500    0.9120    0.9306       125\n",
      "          43     0.8571    0.8400    0.8485       100\n",
      "          44     0.9302    0.9756    0.9524       123\n",
      "          45     0.8462    0.9453    0.8930       128\n",
      "          46     0.9615    0.9766    0.9690       128\n",
      "          47     0.8837    0.8906    0.8872       128\n",
      "          48     0.7075    0.8189    0.7591       127\n",
      "          49     0.9091    0.8800    0.8943       125\n",
      "\n",
      "    accuracy                         0.8420      5847\n",
      "   macro avg     0.8361    0.8231    0.8224      5847\n",
      "weighted avg     0.8422    0.8420    0.8365      5847\n",
      "\n",
      "\n",
      "ðŸ”¹ Processing dataset: flattened_mel_spectrogram_kirigami.csv\n",
      "âœ… Model trained on flattened_mel_spectrogram_kirigami.csv features.\n",
      "ðŸŽ¯ Accuracy: 0.7140\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4519    0.6100    0.5191       100\n",
      "           1     0.6444    0.4833    0.5524        60\n",
      "           2     0.6531    0.5926    0.6214       108\n",
      "           3     0.6832    0.6509    0.6667       106\n",
      "           4     0.7941    0.9153    0.8504       118\n",
      "           5     0.5644    0.5938    0.5787        96\n",
      "           6     0.5214    0.5701    0.5446       107\n",
      "           7     0.7770    0.9310    0.8471       116\n",
      "           8     0.6102    0.6372    0.6234       113\n",
      "           9     0.8021    0.7778    0.7897        99\n",
      "          10     0.8028    0.9500    0.8702       120\n",
      "          11     0.7424    0.8167    0.7778       120\n",
      "          12     0.8295    0.9068    0.8664       118\n",
      "          13     0.9504    0.9829    0.9664       117\n",
      "          14     0.8295    0.9068    0.8664       118\n",
      "          15     0.7204    0.6569    0.6872       102\n",
      "          16     0.7639    0.9167    0.8333       120\n",
      "          17     0.5785    0.6250    0.6009       112\n",
      "          18     0.7000    0.7000    0.7000       120\n",
      "          19     0.6383    0.7500    0.6897       120\n",
      "          20     0.7640    0.6018    0.6733       113\n",
      "          21     0.3187    0.3671    0.3412        79\n",
      "          22     0.6364    0.7368    0.6829       114\n",
      "          23     0.5648    0.6289    0.5951        97\n",
      "          24     0.8929    0.3125    0.4630        80\n",
      "          25     0.6794    0.7739    0.7236       115\n",
      "          26     0.7541    0.4423    0.5576       104\n",
      "          27     0.6335    0.8718    0.7338       117\n",
      "          28     0.6319    0.7647    0.6920       119\n",
      "          29     0.6667    0.3366    0.4474       101\n",
      "          30     0.6750    0.3375    0.4500        80\n",
      "          31     0.7732    0.7282    0.7500       103\n",
      "          32     0.6552    0.6387    0.6468       119\n",
      "          33     0.6620    0.4352    0.5251       108\n",
      "          34     0.6353    0.6207    0.6279        87\n",
      "          35     0.9106    0.9333    0.9218       120\n",
      "          36     0.8240    0.8729    0.8477       118\n",
      "          37     0.9811    0.9123    0.9455       114\n",
      "          38     0.7573    0.6500    0.6996       120\n",
      "          39     0.6923    0.4286    0.5294        63\n",
      "          40     0.9151    0.8291    0.8700       117\n",
      "          41     0.8655    0.8583    0.8619       120\n",
      "          42     0.8200    0.5190    0.6357        79\n",
      "          43     0.7375    0.6344    0.6821        93\n",
      "          44     0.8516    0.9397    0.8934       116\n",
      "          45     0.6364    0.8750    0.7368       120\n",
      "          46     0.6729    0.6667    0.6698       108\n",
      "          47     0.7119    0.7000    0.7059       120\n",
      "          48     0.5852    0.6639    0.6220       119\n",
      "          49     0.8269    0.7288    0.7748       118\n",
      "\n",
      "    accuracy                         0.7140      5371\n",
      "   macro avg     0.7158    0.6956    0.6952      5371\n",
      "weighted avg     0.7208    0.7140    0.7083      5371\n",
      "\n",
      "\n",
      "ðŸ”¹ Processing dataset: flattened_mel_spectrogram_PrivacyMic.csv\n",
      "âœ… Model trained on flattened_mel_spectrogram_PrivacyMic.csv features.\n",
      "ðŸŽ¯ Accuracy: 0.5178\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4000    0.2692    0.3218       104\n",
      "           1     0.4355    0.3103    0.3624        87\n",
      "           2     0.2647    0.1667    0.2045       108\n",
      "           3     0.5250    0.3853    0.4444       109\n",
      "           4     0.5217    0.8000    0.6316       120\n",
      "           5     0.4274    0.5146    0.4670       103\n",
      "           6     0.4184    0.3596    0.3868       114\n",
      "           7     0.5854    0.6261    0.6050       115\n",
      "           8     0.4628    0.4706    0.4667       119\n",
      "           9     0.5588    0.6786    0.6129       112\n",
      "          10     0.4028    0.7083    0.5136       120\n",
      "          11     0.3742    0.5083    0.4311       120\n",
      "          12     0.5723    0.7712    0.6570       118\n",
      "          13     0.5828    0.7333    0.6494       120\n",
      "          14     0.4646    0.7667    0.5786       120\n",
      "          15     0.6889    0.6078    0.6458       102\n",
      "          16     0.5298    0.6667    0.5904       120\n",
      "          17     0.4679    0.6460    0.5428       113\n",
      "          18     0.4887    0.5462    0.5159       119\n",
      "          19     0.3684    0.2333    0.2857       120\n",
      "          20     0.3818    0.3529    0.3668       119\n",
      "          21     0.3824    0.1625    0.2281        80\n",
      "          22     0.5056    0.7692    0.6102       117\n",
      "          23     0.4426    0.2673    0.3333       101\n",
      "          24     0.1667    0.0625    0.0909        80\n",
      "          25     0.4435    0.4783    0.4603       115\n",
      "          26     0.3509    0.1835    0.2410       109\n",
      "          27     0.4154    0.6923    0.5192       117\n",
      "          28     0.5270    0.3277    0.4041       119\n",
      "          29     0.4419    0.1881    0.2639       101\n",
      "          30     0.4286    0.2250    0.2951        80\n",
      "          31     0.5579    0.5146    0.5354       103\n",
      "          32     0.5000    0.4576    0.4779       118\n",
      "          33     0.4762    0.2752    0.3488       109\n",
      "          34     0.5000    0.4138    0.4528        87\n",
      "          35     0.8197    0.8333    0.8264       120\n",
      "          36     0.6096    0.7417    0.6692       120\n",
      "          37     0.6014    0.7217    0.6561       115\n",
      "          38     0.6058    0.6917    0.6459       120\n",
      "          39     0.2273    0.0794    0.1176        63\n",
      "          40     0.6277    0.7350    0.6772       117\n",
      "          41     0.7103    0.6333    0.6696       120\n",
      "          42     0.6207    0.4576    0.5268       118\n",
      "          43     0.6250    0.4688    0.5357        96\n",
      "          44     0.7680    0.8276    0.7967       116\n",
      "          45     0.5180    0.6000    0.5560       120\n",
      "          46     0.7031    0.7500    0.7258       120\n",
      "          47     0.6210    0.6417    0.6311       120\n",
      "          48     0.3333    0.3333    0.3333       120\n",
      "          49     0.6104    0.3983    0.4821       118\n",
      "\n",
      "    accuracy                         0.5178      5521\n",
      "   macro avg     0.5012    0.5011    0.4878      5521\n",
      "weighted avg     0.5083    0.5178    0.5004      5521\n",
      "\n",
      "\n",
      "ðŸ”¹ Processing dataset: flattened_mel_spectrogram_Samosa.csv\n",
      "âœ… Model trained on flattened_mel_spectrogram_Samosa.csv features.\n",
      "ðŸŽ¯ Accuracy: 0.7494\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7167    0.6719    0.6935       128\n",
      "           1     0.6707    0.4297    0.5238       128\n",
      "           2     0.7778    0.6016    0.6784       128\n",
      "           3     0.8269    0.6719    0.7414       128\n",
      "           4     0.8978    0.9609    0.9283       128\n",
      "           5     0.7849    0.5703    0.6606       128\n",
      "           6     0.7612    0.7969    0.7786       128\n",
      "           7     0.8188    0.8828    0.8496       128\n",
      "           8     0.7955    0.8203    0.8077       128\n",
      "           9     0.8433    0.8828    0.8626       128\n",
      "          10     0.8493    0.9688    0.9051       128\n",
      "          11     0.6848    0.8828    0.7713       128\n",
      "          12     0.7881    0.9297    0.8530       128\n",
      "          13     1.0000    1.0000    1.0000       128\n",
      "          14     0.8611    0.9688    0.9118       128\n",
      "          15     0.8632    0.6406    0.7354       128\n",
      "          16     0.9044    0.9609    0.9318       128\n",
      "          17     0.7679    0.6719    0.7167       128\n",
      "          18     0.8203    0.8203    0.8203       128\n",
      "          19     0.7603    0.8672    0.8102       128\n",
      "          20     0.8087    0.7266    0.7654       128\n",
      "          21     0.4462    0.2266    0.3005       128\n",
      "          22     0.8898    0.8203    0.8537       128\n",
      "          23     0.6804    0.5156    0.5867       128\n",
      "          24     0.7632    0.2266    0.3494       128\n",
      "          25     0.6133    0.7188    0.6619       128\n",
      "          26     0.7683    0.4922    0.6000       128\n",
      "          27     0.6855    0.8516    0.7596       128\n",
      "          28     0.7315    0.8516    0.7870       128\n",
      "          29     0.6533    0.3828    0.4828       128\n",
      "          30     0.1625    0.7109    0.2645       128\n",
      "          31     0.8438    0.6328    0.7232       128\n",
      "          32     0.8130    0.7812    0.7968       128\n",
      "          33     0.8841    0.4766    0.6193       128\n",
      "          34     0.7093    0.4766    0.5701       128\n",
      "          35     0.9380    0.9453    0.9416       128\n",
      "          36     0.9302    0.9375    0.9339       128\n",
      "          37     0.9580    0.8906    0.9231       128\n",
      "          38     0.8429    0.9219    0.8806       128\n",
      "          39     0.4492    0.4141    0.4309       128\n",
      "          40     0.9076    0.8438    0.8745       128\n",
      "          41     0.8512    0.8047    0.8273       128\n",
      "          42     0.9083    0.8516    0.8790       128\n",
      "          43     0.8911    0.7031    0.7860       128\n",
      "          44     0.9070    0.9141    0.9105       128\n",
      "          45     0.8194    0.9219    0.8676       128\n",
      "          46     0.9213    0.9141    0.9176       128\n",
      "          47     0.8538    0.8672    0.8605       128\n",
      "          48     0.7013    0.8438    0.7660       128\n",
      "          49     0.9035    0.8047    0.8512       128\n",
      "\n",
      "    accuracy                         0.7494      6400\n",
      "   macro avg     0.7886    0.7494    0.7550      6400\n",
      "weighted avg     0.7886    0.7494    0.7550      6400\n",
      "\n",
      "\n",
      "ðŸ”¹ Processing dataset: flattened_mel_spectrogram_SyntheticSensor.csv\n",
      "âœ… Model trained on flattened_mel_spectrogram_SyntheticSensor.csv features.\n",
      "ðŸŽ¯ Accuracy: 0.7357\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7455    0.7321    0.7387        56\n",
      "           1     0.6970    0.4107    0.5169        56\n",
      "           2     0.7593    0.7321    0.7455        56\n",
      "           3     0.7736    0.7321    0.7523        56\n",
      "           4     0.8485    1.0000    0.9180        56\n",
      "           5     0.7174    0.5893    0.6471        56\n",
      "           6     0.8148    0.7857    0.8000        56\n",
      "           7     0.7667    0.8214    0.7931        56\n",
      "           8     0.6935    0.7679    0.7288        56\n",
      "           9     0.8679    0.8214    0.8440        56\n",
      "          10     0.7857    0.9821    0.8730        56\n",
      "          11     0.6800    0.9107    0.7786        56\n",
      "          12     0.7910    0.9464    0.8618        56\n",
      "          13     0.9333    1.0000    0.9655        56\n",
      "          14     0.8333    0.9821    0.9016        56\n",
      "          15     0.7800    0.6964    0.7358        56\n",
      "          16     0.8462    0.9821    0.9091        56\n",
      "          17     0.8636    0.6786    0.7600        56\n",
      "          18     0.7917    0.6786    0.7308        56\n",
      "          19     0.7000    0.7500    0.7241        56\n",
      "          20     0.8627    0.7857    0.8224        56\n",
      "          21     0.3556    0.2857    0.3168        56\n",
      "          22     0.8947    0.9107    0.9027        56\n",
      "          23     0.6818    0.5357    0.6000        56\n",
      "          24     0.6667    0.1429    0.2353        56\n",
      "          25     0.5522    0.6607    0.6016        56\n",
      "          26     0.7727    0.3036    0.4359        56\n",
      "          27     0.6923    0.8036    0.7438        56\n",
      "          28     0.6727    0.6607    0.6667        56\n",
      "          29     0.5135    0.3393    0.4086        56\n",
      "          30     0.5938    0.3393    0.4318        56\n",
      "          31     0.7826    0.6429    0.7059        56\n",
      "          32     0.6875    0.7857    0.7333        56\n",
      "          33     0.7667    0.4107    0.5349        56\n",
      "          34     0.7000    0.5000    0.5833        56\n",
      "          35     0.9310    0.9643    0.9474        56\n",
      "          36     0.8596    0.8750    0.8673        56\n",
      "          37     1.0000    0.9286    0.9630        56\n",
      "          38     0.8209    0.9821    0.8943        56\n",
      "          39     0.1557    0.5893    0.2463        56\n",
      "          40     0.8750    0.8750    0.8750        56\n",
      "          41     0.9000    0.8036    0.8491        56\n",
      "          42     0.9778    0.7857    0.8713        56\n",
      "          43     0.8974    0.6250    0.7368        56\n",
      "          44     0.9649    0.9821    0.9735        56\n",
      "          45     0.7246    0.8929    0.8000        56\n",
      "          46     0.9123    0.9286    0.9204        56\n",
      "          47     0.8913    0.7321    0.8039        56\n",
      "          48     0.6571    0.8214    0.7302        56\n",
      "          49     0.8772    0.8929    0.8850        56\n",
      "\n",
      "    accuracy                         0.7357      2800\n",
      "   macro avg     0.7666    0.7357    0.7362      2800\n",
      "weighted avg     0.7666    0.7357    0.7362      2800\n",
      "\n",
      "\n",
      "ðŸ“„ Metrics extracted and saved to classification_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# List of all processed feature CSVs\n",
    "csv_files = [\n",
    "     \"flattened_mel_spectrogram_RawAudio.csv\",\n",
    "    \"flattened_mel_spectrogram_kirigami.csv\",\n",
    "    \"flattened_mel_spectrogram_PrivacyMic.csv\",\n",
    "    \"flattened_mel_spectrogram_Samosa.csv\",\n",
    "    \"flattened_mel_spectrogram_SyntheticSensor.csv\"\n",
    "   \n",
    "    \n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "metrics_results = {}\n",
    "\n",
    "# Iterate over each CSV file and train a model\n",
    "for selected_csv in csv_files:\n",
    "    print(f\"\\nðŸ”¹ Processing dataset: {selected_csv}\")\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(selected_csv)\n",
    "\n",
    "    # Ensure Mel-Spectrogram columns exist\n",
    "    feature_cols = [col for col in df.columns if col.startswith(\"mel_\")]\n",
    "    if not feature_cols:\n",
    "        raise ValueError(f\"Error: No 'mel_' feature columns found in {selected_csv}.\")\n",
    "\n",
    "    # Remove rows where all Mel-Spectrogram features are zero\n",
    "    df_filtered = df.loc[~(df[feature_cols] == 0).all(axis=1)]\n",
    "\n",
    "    # Check if the dataset is empty after filtering\n",
    "    if df_filtered.empty:\n",
    "        raise ValueError(f\"Error: After removing zero-only feature rows, {selected_csv} has no data left.\")\n",
    "\n",
    "    X = df_filtered[feature_cols].values  # Feature matrix\n",
    "    y = df_filtered[\"Label\"].values  # Labels\n",
    "\n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Initialize and train the Random Forest classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute Precision, Recall, and F1-score for each class\n",
    "    report = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "\n",
    "    # Store results in dictionary\n",
    "    metrics_results[selected_csv] = {\n",
    "        \"Precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"Recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"F1-score\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"âœ… Model trained on {selected_csv} features.\")\n",
    "    print(f\"ðŸŽ¯ Accuracy: {metrics_results[selected_csv]['Accuracy']:.4f}\\n\")\n",
    "    print(f\"ðŸ“Š Classification Report:\\n{classification_report(y_test, y_pred, digits=4)}\")\n",
    "\n",
    "# Convert metrics to a DataFrame for better visualization\n",
    "df_metrics = pd.DataFrame.from_dict(metrics_results, orient=\"index\")\n",
    "\n",
    "# Save extracted metrics to CSV\n",
    "output_csv = \"classification_metrics_summary.csv\"\n",
    "df_metrics.to_csv(output_csv, index=True)\n",
    "\n",
    "# Display the extracted results\n",
    "print(f\"\\nðŸ“„ Metrics extracted and saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7132c5-0629-4fe2-830d-a2629a2bcc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6446d-4faf-4569-819d-6ba909ade1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee212a1-27b1-4f68-9a4f-a7f2c59ea14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-2022.10]",
   "language": "python",
   "name": "conda-env-anaconda-2022.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
