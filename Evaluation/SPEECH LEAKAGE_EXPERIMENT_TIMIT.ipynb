{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747033e9-eb4b-4729-97b7-8d33e8e22f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from jiwer import wer\n",
    "from pystoi import stoi\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from scipy import signal\n",
    "import librosa\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ee578e-5105-4f36-8403-da2b68550823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1727135/278560748.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/scipy_phoneme_filter.ckpt\", map_location=device))\n",
      "/tmp/ipykernel_1727135/278560748.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/noisy_background_scipy_detector.ckpt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, feature_dim=129):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.sigmoid(self.linear1(torch.nn.functional.normalize(xx, p=1.0, dim=1)))\n",
    "\n",
    "\n",
    "def load_kirigami_model():\n",
    "    # Phoneme Model!\n",
    "    my_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_phoneme_filter_model.load_state_dict(\n",
    "    torch.load(lr_phoneme_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_phoneme_filter_model.eval()\n",
    "\n",
    "    return my_phoneme_filter_model\n",
    "\n",
    "\n",
    "def load_background_filter_model():\n",
    "    # Background Model!\n",
    "    my_background_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_background_filter_model.load_state_dict(torch.load(bg_lr_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_background_filter_model.eval()\n",
    "\n",
    "    return my_background_filter_model\n",
    "\n",
    "\n",
    "def kirigami_filter_torch(s_full, threshold=0.5):\n",
    "    lr_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    # load the model if in kirigami_filters directory\n",
    "    if os.path.exists(\"./kirigami_filters/scipy_phoneme_filter.ckpt\"):\n",
    "        lr_phoneme_filter_model.load_state_dict(\n",
    "            torch.load(\"./kirigami_filters/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Phoneme filter model not found\")\n",
    "\n",
    "    lr_phoneme_filter_model.eval()\n",
    "    pred = (lr_phoneme_filter_model.forward(torch.Tensor(s_full)) >= threshold).long().numpy()\n",
    "    masked = (1 - pred) * s_full\n",
    "    return masked\n",
    "\n",
    "\n",
    "def kirigami_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "\n",
    "        sum = np.sum(fft)\n",
    "\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def kirigami_filter_reverse_fft(stft, stft_original):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            # output_sp[i] = stft[i]\n",
    "            output_sp[i] = stft_original[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def background_detection_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight_background)):\n",
    "            product = product + vv * weight_background[iw]\n",
    "        product = product // sum\n",
    "        product = product + bias_background\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"Background probability: \", i, z)\n",
    "        if z < BACKGROUND_LR_THRESHOLD:  # lower than threshold not background.\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Kirigami models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_kirigami_model():\n",
    "    \"\"\"Load the phoneme filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_background_filter_model():\n",
    "    \"\"\"Load the background noise filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/noisy_background_scipy_detector.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "phoneme_filter_model = load_kirigami_model()\n",
    "background_filter_model = load_background_filter_model()\n",
    "\n",
    "# Extract model weights\n",
    "weight_phoneme = phoneme_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_phoneme = phoneme_filter_model.linear1.bias.data[0].numpy()\n",
    "weight_background = background_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_background = background_filter_model.linear1.bias.data[0].numpy()\n",
    "\n",
    "\n",
    "\n",
    "def apply_kirigami_filter(stft, weight, bias, threshold=0.5):\n",
    "    \"\"\"Apply Kirigami logistic regression filter on STFT features (ensuring 129 features per frame).\"\"\"\n",
    "    output_stft = np.zeros_like(stft)\n",
    "\n",
    "    for i, frame in enumerate(stft):\n",
    "        frame = frame[:129]  # Ensure exactly 129 dimensions\n",
    "\n",
    "        sum_val = np.sum(frame) + 1e-6  # Avoid division by zero\n",
    "        product = np.dot(frame, weight) / sum_val + bias\n",
    "        prob = 1 / (1 + np.exp(-product))  # Sigmoid activation\n",
    "\n",
    "        if prob < threshold:  # If probability is low, keep frame\n",
    "            output_stft[i, :129] = frame  # Apply only to the valid region\n",
    "\n",
    "    return output_stft\n",
    "\n",
    "# Process Audio File\n",
    "def Kirigami_process_audio( audio, sr=16000, threshold=0.5):\n",
    "    \"\"\"Process an audio file through Kirigami models and save the filtered output.\"\"\"\n",
    "    # Load audio\n",
    "    # audio, sr = librosa.load(input_audio_path, sr=16000)\n",
    "\n",
    "    # Compute STFT (Ensure output has 129 feature bins)\n",
    "    stft = np.abs(librosa.stft(audio, n_fft=256, hop_length=128))[:129, :].T  # Transpose for correct shape\n",
    "\n",
    "    # Apply Kirigami phoneme & background filters\n",
    "    filtered_stft_phoneme = apply_kirigami_filter(stft, weight_phoneme, bias_phoneme, threshold)\n",
    "    # filtered_stft_background = self.apply_kirigami_filter(filtered_stft_phoneme, weight_background, bias_background, threshold)\n",
    "\n",
    "    # Convert back to audio using inverse STFT\n",
    "    filtered_audio = librosa.istft(filtered_stft_phoneme.T, hop_length=128)\n",
    "\n",
    "    return filtered_audio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784bad4f-e159-4be8-9ee1-47b6cdd59a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading facebook/wav2vec2-base-960h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgomasta_umass_edu/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing method: raw\n",
      "\n",
      "Method: Raw Audio\n",
      "Average WER: 12.84%\n",
      "Average PER: 17.38%\n",
      "Average ESTOI: 100.00%\n",
      "\n",
      "Processing method: samosa\n",
      "\n",
      "Method: SAMOSA\n",
      "Average WER: 98.28%\n",
      "Average PER: 79.87%\n",
      "Average ESTOI: 33.26%\n",
      "\n",
      "Processing method: synthetic_sensor\n",
      "\n",
      "Method: Synthetic Sensor\n",
      "Average WER: 100.82%\n",
      "Average PER: 83.08%\n",
      "Average ESTOI: 24.62%\n",
      "\n",
      "Processing method: privacymic\n",
      "\n",
      "Method: PrivacyMic\n",
      "Average WER: 100.00%\n",
      "Average PER: 100.00%\n",
      "Average ESTOI: 15.58%\n",
      "\n",
      "Processing method: coughsense_withphase\n",
      "\n",
      "Method: CoughSense (with phase)\n",
      "Average WER: 26.05%\n",
      "Average PER: 23.95%\n",
      "Average ESTOI: 80.23%\n",
      "\n",
      "Processing method: coughsense_withoutphase\n",
      "\n",
      "Method: CoughSense (without phase)\n",
      "Average WER: 98.75%\n",
      "Average PER: 79.80%\n",
      "Average ESTOI: 17.73%\n",
      "\n",
      "Processing method: Kirigami_process_Audio\n",
      "\n",
      "Method: Kirigami\n",
      "Average WER: 12.84%\n",
      "Average PER: 17.38%\n",
      "Average ESTOI: 100.00%\n",
      "\n",
      "Final Summary of Results:\n",
      "\n",
      "Method: Raw Audio\n",
      "WER: 12.84%\n",
      "PER: 17.38%\n",
      "ESTOI: 100.00%\n",
      "\n",
      "Method: SAMOSA\n",
      "WER: 98.28%\n",
      "PER: 79.87%\n",
      "ESTOI: 33.26%\n",
      "\n",
      "Method: Synthetic Sensor\n",
      "WER: 100.82%\n",
      "PER: 83.08%\n",
      "ESTOI: 24.62%\n",
      "\n",
      "Method: PrivacyMic\n",
      "WER: 100.00%\n",
      "PER: 100.00%\n",
      "ESTOI: 15.58%\n",
      "\n",
      "Method: CoughSense (with phase)\n",
      "WER: 26.05%\n",
      "PER: 23.95%\n",
      "ESTOI: 80.23%\n",
      "\n",
      "Method: CoughSense (without phase)\n",
      "WER: 98.75%\n",
      "PER: 79.80%\n",
      "ESTOI: 17.73%\n",
      "\n",
      "Method: Kirigami\n",
      "WER: 12.84%\n",
      "PER: 17.38%\n",
      "ESTOI: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "\n",
    "class TIMITEvaluator:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.cmudict = cmudict.dict()\n",
    "        except LookupError:\n",
    "            nltk.download('cmudict')\n",
    "            self.cmudict = cmudict.dict()\n",
    "        \n",
    "        model_name = \"facebook/wav2vec2-base-960h\"\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "        self.phoneme_map = {\n",
    "            'aa': 'aa', 'ae': 'ae', 'ah': 'ah', 'ao': 'aa', 'aw': 'aw',\n",
    "            'ax': 'ah', 'ax-h': 'ah', 'axr': 'er', 'ay': 'ay', 'b': 'b',\n",
    "            'bcl': 'b', 'ch': 'ch', 'd': 'd', 'dcl': 'd', 'dh': 'dh',\n",
    "            'dx': 't', 'eh': 'eh', 'el': 'l', 'em': 'm', 'en': 'n',\n",
    "            'eng': 'ng', 'epi': '', 'er': 'er', 'ey': 'ey', 'f': 'f',\n",
    "            'g': 'g', 'gcl': 'g', 'h#': '', 'hh': 'hh', 'hv': 'hh',\n",
    "            'ih': 'ih', 'ix': 'ih', 'iy': 'iy', 'jh': 'jh', 'k': 'k',\n",
    "            'kcl': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'ng': 'ng',\n",
    "            'nx': 'n', 'ow': 'ow', 'oy': 'oy', 'p': 'p', 'pau': '',\n",
    "            'pcl': 'p', 'q': '', 'r': 'r', 's': 's', 'sh': 'sh',\n",
    "            't': 't', 'tcl': 't', 'th': 'th', 'uh': 'uh', 'uw': 'uw',\n",
    "            'ux': 'uw', 'v': 'v', 'w': 'w', 'y': 'y', 'z': 'z', 'zh': 'zh'\n",
    "        }\n",
    "\n",
    "        self.cmu_to_timit = {\n",
    "            'AA': 'aa', 'AE': 'ae', 'AH': 'ah', 'AO': 'aa', 'AW': 'aw',\n",
    "            'AY': 'ay', 'B': 'b', 'CH': 'ch', 'D': 'd', 'DH': 'dh',\n",
    "            'EH': 'eh', 'ER': 'er', 'EY': 'ey', 'F': 'f', 'G': 'g',\n",
    "            'HH': 'hh', 'IH': 'ih', 'IY': 'iy', 'JH': 'jh', 'K': 'k',\n",
    "            'L': 'l', 'M': 'm', 'N': 'n', 'NG': 'ng', 'OW': 'ow',\n",
    "            'OY': 'oy', 'P': 'p', 'R': 'r', 'S': 's', 'SH': 'sh',\n",
    "            'T': 't', 'TH': 'th', 'UH': 'uh', 'UW': 'uw', 'V': 'v',\n",
    "            'W': 'w', 'Y': 'y', 'Z': 'z', 'ZH': 'zh'\n",
    "        }\n",
    "\n",
    "    def get_phonemes(self, phn_file):\n",
    "        phonemes = []\n",
    "        with open(phn_file, 'r') as f:\n",
    "            for line in f:\n",
    "                _, _, phone = line.strip().split()\n",
    "                phone = phone.lower()\n",
    "                mapped_phone = self.phoneme_map.get(phone, phone)\n",
    "                if mapped_phone:\n",
    "                    phonemes.append(mapped_phone)\n",
    "        return self.normalize_phoneme_sequence(phonemes)\n",
    "\n",
    "    def normalize_phoneme_sequence(self, phonemes):\n",
    "        phonemes = [p for p in phonemes if p]\n",
    "        normalized = []\n",
    "        for i, phone in enumerate(phonemes):\n",
    "            if i == 0 or phone != phonemes[i-1]:\n",
    "                normalized.append(phone)\n",
    "        return normalized\n",
    "\n",
    "    def convert_to_phonemes(self, text):\n",
    "        phones = []\n",
    "        for word in text.lower().split():\n",
    "            if word in self.cmudict:\n",
    "                word_phones = self.cmudict[word][0]\n",
    "                timit_phones = [self.cmu_to_timit[p.rstrip('0123456789')] \n",
    "                              for p in word_phones]\n",
    "                phones.extend(timit_phones)\n",
    "            else:\n",
    "                char_phones = self.convert_word_to_phonemes(word)\n",
    "                phones.extend(char_phones)\n",
    "        return self.normalize_phoneme_sequence(phones)\n",
    "\n",
    "    def convert_word_to_phonemes(self, word):\n",
    "        phones = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1:\n",
    "                digraph = word[i:i+2]\n",
    "                if digraph in ['th', 'ch', 'sh', 'ph', 'wh', 'gh']:\n",
    "                    phones.append('th' if digraph == 'th' else\n",
    "                                'ch' if digraph == 'ch' else\n",
    "                                'sh' if digraph == 'sh' else\n",
    "                                'f' if digraph == 'ph' else\n",
    "                                'w' if digraph == 'wh' else 'g')\n",
    "                    i += 2\n",
    "                    continue\n",
    "            \n",
    "            c = word[i]\n",
    "            if c in 'aeiou':\n",
    "                phones.append('ae' if c == 'a' else\n",
    "                            'eh' if c == 'e' else\n",
    "                            'ih' if c == 'i' else\n",
    "                            'ow' if c == 'o' else 'uh')\n",
    "            elif c in self.phoneme_map:\n",
    "                phones.append(self.phoneme_map[c])\n",
    "            elif c in 'bcdfghjklmnpqrstvwxyz':\n",
    "                phones.append(c)\n",
    "            i += 1\n",
    "        return phones\n",
    "\n",
    "    def calculate_per(self, ref_phones, pred_phones):\n",
    "        if not ref_phones:\n",
    "            return 0.0\n",
    "\n",
    "        ref_phones = self.normalize_phoneme_sequence(ref_phones)\n",
    "        pred_phones = self.normalize_phoneme_sequence(pred_phones)\n",
    "\n",
    "        R = len(ref_phones)\n",
    "        H = len(pred_phones)\n",
    "        D = np.zeros((R + 1, H + 1))\n",
    "        \n",
    "        for i in range(R + 1):\n",
    "            D[i, 0] = i\n",
    "        for j in range(H + 1):\n",
    "            D[0, j] = j\n",
    "        \n",
    "        for i in range(1, R + 1):\n",
    "            for j in range(1, H + 1):\n",
    "                if ref_phones[i-1] == pred_phones[j-1]:\n",
    "                    D[i, j] = D[i-1, j-1]\n",
    "                else:\n",
    "                    sub_cost = 1.0\n",
    "                    if self.are_similar_phones(ref_phones[i-1], pred_phones[j-1]):\n",
    "                        sub_cost = 0.5\n",
    "                    D[i, j] = min(D[i-1, j] + 1.0,\n",
    "                                D[i, j-1] + 1.0,\n",
    "                                D[i-1, j-1] + sub_cost)\n",
    "        \n",
    "        return (D[R, H] / R) * 100 if R else 0.0\n",
    "\n",
    "    def are_similar_phones(self, phone1, phone2):\n",
    "        groups = [\n",
    "            {'aa', 'ae', 'ah', 'ao', 'aw', 'ay', 'eh', 'er', 'ey', 'ih', 'iy', 'ow', 'oy', 'uh', 'uw'},\n",
    "            {'b', 'p', 'd', 't', 'g', 'k'},\n",
    "            {'f', 'v', 'th', 'dh', 's', 'z', 'sh', 'zh', 'hh'},\n",
    "            {'m', 'n', 'ng'},\n",
    "            {'l', 'r'},\n",
    "            {'w', 'y'}\n",
    "        ]\n",
    "        return any(phone1 in group and phone2 in group for group in groups)\n",
    "\n",
    "    def apply_samosa(self, audio, sr=16000):\n",
    "        subsampled_audio = signal.resample(audio, int(len(audio) * 1000 / sr))\n",
    "        upsampled_audio = signal.resample(subsampled_audio, len(audio))\n",
    "        return upsampled_audio\n",
    "\n",
    "    def apply_synthetic_sensor(self, audio, sr=16000):\n",
    "        window_length = 256\n",
    "        hop_length = 128\n",
    "        samples_per_window = 10\n",
    "\n",
    "        frames = [\n",
    "            audio[i:i + window_length]\n",
    "            for i in range(0, len(audio) - window_length + 1, hop_length)\n",
    "        ]\n",
    "        \n",
    "        processed_audio = np.zeros(len(audio))\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            fft = np.fft.fft(frame, n=window_length)\n",
    "            bins = np.array_split(fft, samples_per_window)\n",
    "            reduced_fft = np.array([np.mean(bin) for bin in bins])\n",
    "            reconstructed_fft = np.zeros_like(fft, dtype=np.complex_)\n",
    "            step = len(fft) // len(reduced_fft)\n",
    "            for j, val in enumerate(reduced_fft):\n",
    "                reconstructed_fft[j * step:(j + 1) * step] = val\n",
    "            processed_frame = np.fft.ifft(reconstructed_fft).real\n",
    "            start = i * hop_length\n",
    "            end = min(start + window_length, len(processed_audio))\n",
    "            processed_audio[start:end] += processed_frame[:end - start]\n",
    "        \n",
    "        if np.max(np.abs(processed_audio)) > 0:\n",
    "            processed_audio = processed_audio / np.max(np.abs(processed_audio))\n",
    "            \n",
    "        return processed_audio\n",
    "\n",
    "    def apply_privacymic(self, audio, sr=16000):\n",
    "        S = librosa.stft(audio, n_fft=256, hop_length=128)\n",
    "        frequencies = librosa.fft_frequencies(sr=sr, n_fft=256)\n",
    "        S_filtered = np.where(frequencies[:, None] <= 300, S, 0)\n",
    "        return librosa.istft(S_filtered, hop_length=128)\n",
    "    \n",
    "    def apply_kirigami_filter(self, stft, weight, bias, threshold=0.5):\n",
    "        \"\"\"Apply Kirigami logistic regression filter on STFT features (ensuring 129 features per frame).\"\"\"\n",
    "        output_stft = np.zeros_like(stft)\n",
    "\n",
    "        for i, frame in enumerate(stft):\n",
    "            frame = frame[:129]  # Ensure exactly 129 dimensions\n",
    "\n",
    "            sum_val = np.sum(frame) + 1e-6  # Avoid division by zero\n",
    "            product = np.dot(frame, weight) / sum_val + bias\n",
    "            prob = 1 / (1 + np.exp(-product))  # Sigmoid activation\n",
    "\n",
    "            if prob < threshold:  # If probability is low, keep frame\n",
    "                output_stft[i, :129] = frame  # Apply only to the valid region\n",
    "\n",
    "        return output_stft\n",
    "\n",
    "    # Process Audio File\n",
    "    def Kirigami_process_audio(self,  audio, sr=16000, threshold=0.5):\n",
    "        \"\"\"Process an audio file through Kirigami models and save the filtered output.\"\"\"\n",
    "        # Load audio\n",
    "        # audio, sr = librosa.load(input_audio_path, sr=16000)\n",
    "\n",
    "        # Compute STFT (Ensure output has 129 feature bins)\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=256, hop_length=128))[:129, :].T  # Transpose for correct shape\n",
    "\n",
    "        # Apply Kirigami phoneme & background filters\n",
    "        filtered_stft_phoneme = self.apply_kirigami_filter(stft, weight_phoneme, bias_phoneme, threshold)\n",
    "        # filtered_stft_background = self.apply_kirigami_filter(filtered_stft_phoneme, weight_background, bias_background, threshold)\n",
    "\n",
    "        # Convert back to audio using inverse STFT\n",
    "        filtered_audio = librosa.istft(filtered_stft_phoneme.T, hop_length=128)\n",
    "\n",
    "        return filtered_audio\n",
    "\n",
    "    def apply_coughsense_withphase(self, audio, sr=16000):\n",
    "        \"\"\"CoughSense filtering with phase preservation\"\"\"\n",
    "        # Calculate window size for 150ms window\n",
    "        window_size = int(0.150 * sr)  # 150ms window in samples\n",
    "        hop_length = window_size//2  # stride size\n",
    "        \n",
    "        # Compute STFT\n",
    "        S = librosa.stft(audio, n_fft=window_size, hop_length=hop_length, win_length=window_size)\n",
    "        \n",
    "        # Get magnitude spectrogram\n",
    "        mag_spec = np.abs(S)\n",
    "        \n",
    "        # Reshape for PCA\n",
    "        features = mag_spec.T\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=10)\n",
    "        reduced_features = pca.fit_transform(features)\n",
    "        \n",
    "        # Reconstruct\n",
    "        reconstructed_features = pca.inverse_transform(reduced_features)\n",
    "        \n",
    "        # Reshape back to STFT format\n",
    "        reconstructed_stft = reconstructed_features.T\n",
    "        \n",
    "        # Preserve phase information\n",
    "        phase = np.angle(S)\n",
    "        reconstructed_complex = reconstructed_stft * np.exp(1j * phase)\n",
    "        \n",
    "        # Inverse STFT\n",
    "        filtered_audio = librosa.istft(reconstructed_complex, \n",
    "                                     hop_length=hop_length, \n",
    "                                     win_length=window_size)\n",
    "        \n",
    "        # Ensure output length matches input\n",
    "        if len(filtered_audio) > len(audio):\n",
    "            filtered_audio = filtered_audio[:len(audio)]\n",
    "        elif len(filtered_audio) < len(audio):\n",
    "            filtered_audio = np.pad(filtered_audio, (0, len(audio) - len(filtered_audio)))\n",
    "            \n",
    "        return filtered_audio\n",
    "\n",
    "    def apply_coughsense_withoutphase(self, audio, sr=16000):\n",
    "        \"\"\"CoughSense filtering without phase preservation\"\"\"\n",
    "        # Use a 150 ms window and 50% overlap\n",
    "        n_fft = int(0.15 * sr)  # 150 ms window\n",
    "        hop_length = n_fft // 2  # 50% overlap\n",
    "        \n",
    "        # Compute spectrogram\n",
    "        S = np.abs(librosa.stft(audio, n_fft=n_fft, hop_length=hop_length, window='hamming'))\n",
    "        \n",
    "        # Flatten and apply PCA\n",
    "        S_flattened = S.T  # Transpose for PCA\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=10)  # Retain 10 components\n",
    "        S_reduced = pca.fit_transform(S_flattened)\n",
    "        S_reconstructed = pca.inverse_transform(S_reduced).T\n",
    "        \n",
    "        # Inverse STFT\n",
    "        filtered_audio = librosa.istft(S_reconstructed, \n",
    "                                     hop_length=hop_length, \n",
    "                                     win_length=n_fft)\n",
    "                                     \n",
    "        # Ensure output length matches input\n",
    "        if len(filtered_audio) > len(audio):\n",
    "            filtered_audio = filtered_audio[:len(audio)]\n",
    "        elif len(filtered_audio) < len(audio):\n",
    "            filtered_audio = np.pad(filtered_audio, (0, len(audio) - len(filtered_audio)))\n",
    "            \n",
    "        return filtered_audio\n",
    "\n",
    "    def calculate_estoi(self, clean_audio, processed_audio, sr=16000):\n",
    "        \"\"\"Calculate extended Short-Time Objective Intelligibility (eSTOI).\"\"\"\n",
    "        min_length = min(len(clean_audio), len(processed_audio))\n",
    "        clean_audio = clean_audio[:min_length]\n",
    "        processed_audio = processed_audio[:min_length]\n",
    "        return stoi(clean_audio, processed_audio, sr, extended=True)\n",
    "\n",
    "\n",
    "    def process_file(self, wav_path, method=None):\n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, sr = torchaudio.load(wav_path)\n",
    "            audio = torch.mean(audio, dim=0) if len(audio.shape) > 1 else audio\n",
    "            if sr != self.sample_rate:\n",
    "                audio = torchaudio.functional.resample(audio, sr, self.sample_rate)\n",
    "            audio = audio.numpy()\n",
    "\n",
    "            # Store original audio for ESTOI calculation\n",
    "            original_audio = audio.copy()\n",
    "\n",
    "            # Apply processing method\n",
    "            if method == \"samosa\":\n",
    "                processed_audio = self.apply_samosa(audio, self.sample_rate)\n",
    "            elif method == \"synthetic_sensor\":\n",
    "                processed_audio = self.apply_synthetic_sensor(audio, self.sample_rate)\n",
    "            elif method == \"privacymic\":\n",
    "                processed_audio = self.apply_privacymic(audio, self.sample_rate)\n",
    "            elif method == \"coughsense_withphase\":\n",
    "                processed_audio = self.apply_coughsense_withphase(audio, self.sample_rate)\n",
    "            elif method == \"coughsense_withoutphase\":\n",
    "                processed_audio = self.apply_coughsense_withoutphase(audio, self.sample_rate)\n",
    "            elif method == \"Kirigami_process_file\":\n",
    "                processed_audio = self.Kirigami_process_audio(audio )\n",
    "            else:\n",
    "                processed_audio = audio\n",
    "\n",
    "            # Calculate ESTOI\n",
    "            sample_estoi = self.calculate_estoi(original_audio, processed_audio, self.sample_rate) * 100\n",
    "\n",
    "            # Get ASR prediction using processed audio\n",
    "            inputs = self.processor(processed_audio, sampling_rate=self.sample_rate, \n",
    "                                  return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(inputs.input_values)\n",
    "                pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "                pred_text = self.processor.batch_decode(pred_ids)[0].lower()\n",
    "\n",
    "            # Get reference transcriptions\n",
    "            phn_path = wav_path.replace('.WAV', '.PHN')\n",
    "            wrd_path = wav_path.replace('.WAV', '.WRD')\n",
    "            \n",
    "            # Get reference phonemes and text\n",
    "            ref_phones = self.get_phonemes(phn_path)\n",
    "            with open(wrd_path, 'r') as f:\n",
    "                true_text = ' '.join([line.strip().split()[-1] for line in f]).lower()\n",
    "\n",
    "            # Convert predicted text to phonemes\n",
    "            pred_phones = self.convert_to_phonemes(pred_text)\n",
    "\n",
    "            # Calculate metrics\n",
    "            sample_wer = wer(true_text, pred_text) * 100\n",
    "            sample_per = self.calculate_per(ref_phones, pred_phones)\n",
    "\n",
    "            return {\n",
    "                \"file\": os.path.basename(wav_path),\n",
    "                \"true_text\": true_text,\n",
    "                \"pred_text\": pred_text,\n",
    "                \"ref_phones\": ref_phones,\n",
    "                \"pred_phones\": pred_phones,\n",
    "                \"wer\": sample_wer,\n",
    "                \"per\": sample_per,\n",
    "                \"estoi\": sample_estoi\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {wav_path}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def process_dataset(self, dataset_path, method=None):\n",
    "        results = []\n",
    "\n",
    "        for root, _, files in os.walk(dataset_path):\n",
    "            for file in sorted(files):\n",
    "                if file.endswith('.WAV'):\n",
    "                    if file_count>=max_files:\n",
    "                        return results\n",
    "                    wav_path = os.path.join(root, file)\n",
    "                    result = self.process_file(wav_path, method)\n",
    "                    if result:\n",
    "                        results.append(result)\n",
    "                    \n",
    "        return results\n",
    "    \n",
    "def main():\n",
    "    data_path = '/work/pi_shenoy_umass_edu/sgomasta_umass_edu/WER-PER/TIMIT/data/'\n",
    "    evaluator = TIMITEvaluator()\n",
    "\n",
    "    methods = [None, \"samosa\", \"synthetic_sensor\", \"privacymic\", \n",
    "              \"coughsense_withphase\", \"coughsense_withoutphase\", \"Kirigami_process_Audio\"]\n",
    "    results_summary = {}\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"\\nProcessing method: {method or 'raw'}\")\n",
    "        results = evaluator.process_dataset(data_path, method)\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        if not df.empty:\n",
    "            method_name = method if method else \"Raw Audio\"\n",
    "            if method in {\n",
    "                \"Kirigami_process_Audio\": \"Kirigami\",\n",
    "                \"samosa\": \"SAMOSA\",\n",
    "                \"synthetic_sensor\": \"Synthetic Sensor\",\n",
    "                \"privacymic\": \"PrivacyMic\",\n",
    "                \"coughsense_withphase\": \"CoughSense (with phase)\",\n",
    "                \"coughsense_withoutphase\": \"CoughSense (without phase)\"\n",
    "            }:\n",
    "                method_name = {\n",
    "                    \"Kirigami_process_Audio\": \"Kirigami\",\n",
    "                    \"samosa\": \"SAMOSA\",\n",
    "                    \"synthetic_sensor\": \"Synthetic Sensor\",\n",
    "                    \"privacymic\": \"PrivacyMic\",\n",
    "                    \"coughsense_withphase\": \"CoughSense (with phase)\",\n",
    "                    \"coughsense_withoutphase\": \"CoughSense (without phase)\"\n",
    "                }[method]\n",
    "            \n",
    "            avg_wer = df['wer'].mean()\n",
    "            avg_per = df['per'].mean()\n",
    "            avg_estoi = df['estoi'].mean()\n",
    "            \n",
    "            print(f\"\\nMethod: {method_name}\")\n",
    "            print(f\"Average WER: {avg_wer:.2f}%\")\n",
    "            print(f\"Average PER: {avg_per:.2f}%\")\n",
    "            print(f\"Average ESTOI: {avg_estoi:.2f}%\")\n",
    "            \n",
    "            results_summary[method_name] = {\n",
    "                'WER': avg_wer,\n",
    "                'PER': avg_per,\n",
    "                'ESTOI': avg_estoi\n",
    "            }\n",
    "\n",
    "    print(\"\\nFinal Summary of Results:\")\n",
    "    for method_name, metrics in results_summary.items():\n",
    "        print(f\"\\nMethod: {method_name}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed307f-4bea-4538-a95d-f1e44b48d196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04c16a-64d6-41c6-b546-6e79f0de556f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
