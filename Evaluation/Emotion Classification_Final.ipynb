{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4d6b53-dd79-4fd6-9d29-cd2743f22337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: librosa in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: scipy in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (1.15.1)\n",
      "Requirement already satisfied: tqdm in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: imbalanced-learn in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: torch in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: jiwer in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: pystoi in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: nltk in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: filelock in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from triton==3.3.0->torch) (65.6.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from pooch>=1.1->librosa) (2.6.2)\n",
      "Requirement already satisfied: six>=1.5 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sgomasta_umass_edu/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: pycparser in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install numpy pandas librosa scipy tqdm scikit-learn imbalanced-learn torch torchaudio transformers jiwer pystoi nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ab6589-d8dc-4dac-bae7-dd3b496bdf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from jiwer import wer\n",
    "from pystoi import stoi\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from scipy import signal\n",
    "import librosa\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c9ed7f-7bb6-4542-a14e-d4b851fcf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class AudioFeatureExtractor(ABC):\n",
    "    def __init__(self, n_mfcc=20, max_pad_length=1000):\n",
    "        self.sr = None\n",
    "        self.n_mfcc = n_mfcc\n",
    "    \n",
    "    def extract_mfcc(self, processed_audio):\n",
    "        window_size = int(0.5 * self.sr)\n",
    "        step_size = int(0.25 * self.sr)\n",
    "        mfcc_list = []\n",
    "    \n",
    "        for start in range(0, len(processed_audio) - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window = processed_audio[start:end]\n",
    "            \n",
    "            if len(window) != window_size:\n",
    "                continue\n",
    "    \n",
    "            mfcc = librosa.feature.melspectrogram(y=window, sr=self.sr)\n",
    "            mfcc_list.append(mfcc.mean(axis=1))\n",
    "    \n",
    "        return mfcc_list\n",
    "        \n",
    "    @abstractmethod\n",
    "    def process_audio(self, audio):\n",
    "        pass\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path)\n",
    "            self.sr = sr\n",
    "            processed_audio = self.process_audio(audio)\n",
    "            return self.extract_mfcc(processed_audio)\n",
    "        except Exception as e:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d927ca-8dee-49c9-bac5-e4c82b9e4774",
   "metadata": {},
   "source": [
    "<h1> Binary Classification </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab282b0-49f6-4448-a0dd-ab2fdfc2f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with kirigami extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:38<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for kirigami - Emotion: 0.7499\n",
      "\n",
      "Classification Report for kirigami - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.78      0.68      0.73      1753\n",
      "         sad       0.72      0.82      0.77      1809\n",
      "\n",
      "    accuracy                           0.75      3562\n",
      "   macro avg       0.75      0.75      0.75      3562\n",
      "weighted avg       0.75      0.75      0.75      3562\n",
      "\n",
      "\n",
      "Processing with coughsense extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:15<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for coughsense - Emotion: 0.8605\n",
      "\n",
      "Classification Report for coughsense - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.87      0.85      0.86      1759\n",
      "         sad       0.85      0.87      0.86      1775\n",
      "\n",
      "    accuracy                           0.86      3534\n",
      "   macro avg       0.86      0.86      0.86      3534\n",
      "weighted avg       0.86      0.86      0.86      3534\n",
      "\n",
      "\n",
      "Processing with raw extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:17<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for raw - Emotion: 0.8072\n",
      "\n",
      "Classification Report for raw - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.86      0.75      0.80      1836\n",
      "         sad       0.77      0.87      0.81      1742\n",
      "\n",
      "    accuracy                           0.81      3578\n",
      "   macro avg       0.81      0.81      0.81      3578\n",
      "weighted avg       0.81      0.81      0.81      3578\n",
      "\n",
      "\n",
      "Processing with synthetic extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [03:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for synthetic - Emotion: 0.7887\n",
      "\n",
      "Classification Report for synthetic - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.79      0.80      0.79      1836\n",
      "         sad       0.78      0.78      0.78      1742\n",
      "\n",
      "    accuracy                           0.79      3578\n",
      "   macro avg       0.79      0.79      0.79      3578\n",
      "weighted avg       0.79      0.79      0.79      3578\n",
      "\n",
      "\n",
      "Processing with privacy extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:24<00:00, 23.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for privacy - Emotion: 0.7405\n",
      "\n",
      "Classification Report for privacy - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.79      0.64      0.71      1746\n",
      "         sad       0.71      0.84      0.77      1823\n",
      "\n",
      "    accuracy                           0.74      3569\n",
      "   macro avg       0.75      0.74      0.74      3569\n",
      "weighted avg       0.75      0.74      0.74      3569\n",
      "\n",
      "\n",
      "Processing with samosa extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:33<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for samosa - Emotion: 0.7700\n",
      "\n",
      "Classification Report for samosa - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.80      0.73      0.76      1836\n",
      "         sad       0.74      0.81      0.77      1742\n",
      "\n",
      "    accuracy                           0.77      3578\n",
      "   macro avg       0.77      0.77      0.77      3578\n",
      "weighted avg       0.77      0.77      0.77      3578\n",
      "\n",
      "\n",
      "=== ACCURACY SUMMARY ===\n",
      "\n",
      "Kirigami:\n",
      "  Emotion: 74.99%\n",
      "\n",
      "Coughsense:\n",
      "  Emotion: 86.05%\n",
      "\n",
      "Raw:\n",
      "  Emotion: 80.72%\n",
      "\n",
      "Synthetic:\n",
      "  Emotion: 78.87%\n",
      "\n",
      "Privacy:\n",
      "  Emotion: 74.05%\n",
      "\n",
      "Samosa:\n",
      "  Emotion: 77.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "    \n",
    "\n",
    "class AudioFeatureExtractor(ABC):\n",
    "    def __init__(self, n_mfcc=20, max_pad_length=1000):\n",
    "        self.sr = None\n",
    "        self.n_mfcc = n_mfcc\n",
    "    \n",
    "    def extract_mfcc(self, processed_audio):\n",
    "        window_size = int(0.5 * self.sr)\n",
    "        step_size = int(0.25 * self.sr)\n",
    "        mfcc_list = []\n",
    "    \n",
    "        for start in range(0, len(processed_audio) - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window = processed_audio[start:end]\n",
    "            \n",
    "            if len(window) != window_size:\n",
    "                continue\n",
    "    \n",
    "            mfcc = librosa.feature.melspectrogram(y=window, sr=self.sr)\n",
    "            mfcc_list.append(mfcc.mean(axis=1))\n",
    "    \n",
    "        return mfcc_list\n",
    "        \n",
    "    @abstractmethod\n",
    "    def process_audio(self, audio):\n",
    "        pass\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path)\n",
    "            self.sr = sr\n",
    "            processed_audio = self.process_audio(audio)\n",
    "            return self.extract_mfcc(processed_audio)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        \n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from init_config import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, feature_dim=129):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.sigmoid(self.linear1(torch.nn.functional.normalize(xx, p=1.0, dim=1)))\n",
    "\n",
    "\n",
    "def load_kirigami_model():\n",
    "    # Phoneme Model!\n",
    "    my_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_phoneme_filter_model.load_state_dict(\n",
    "    torch.load(lr_phoneme_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_phoneme_filter_model.eval()\n",
    "\n",
    "    return my_phoneme_filter_model\n",
    "\n",
    "\n",
    "def load_background_filter_model():\n",
    "    # Background Model!\n",
    "    my_background_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_background_filter_model.load_state_dict(torch.load(bg_lr_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_background_filter_model.eval()\n",
    "\n",
    "    return my_background_filter_model\n",
    "\n",
    "\n",
    "def kirigami_filter_torch(s_full, threshold=0.5):\n",
    "    lr_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    # load the model if in kirigami_filters directory\n",
    "    if os.path.exists(\"./kirigami_filters/scipy_phoneme_filter.ckpt\"):\n",
    "        lr_phoneme_filter_model.load_state_dict(\n",
    "            torch.load(\"./kirigami_filters/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Phoneme filter model not found\")\n",
    "\n",
    "    lr_phoneme_filter_model.eval()\n",
    "    pred = (lr_phoneme_filter_model.forward(torch.Tensor(s_full)) >= threshold).long().numpy()\n",
    "    masked = (1 - pred) * s_full\n",
    "    return masked\n",
    "\n",
    "\n",
    "def kirigami_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "\n",
    "        sum = np.sum(fft)\n",
    "\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def kirigami_filter_reverse_fft(stft, stft_original):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            # output_sp[i] = stft[i]\n",
    "            output_sp[i] = stft_original[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def background_detection_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight_background)):\n",
    "            product = product + vv * weight_background[iw]\n",
    "        product = product // sum\n",
    "        product = product + bias_background\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"Background probability: \", i, z)\n",
    "        if z < BACKGROUND_LR_THRESHOLD:  # lower than threshold not background.\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Kirigami models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_kirigami_model():\n",
    "    \"\"\"Load the phoneme filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_background_filter_model():\n",
    "    \"\"\"Load the background noise filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/noisy_background_scipy_detector.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "phoneme_filter_model = load_kirigami_model()\n",
    "background_filter_model = load_background_filter_model()\n",
    "\n",
    "# Extract model weights\n",
    "weight_phoneme = phoneme_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_phoneme = phoneme_filter_model.linear1.bias.data[0].numpy()\n",
    "weight_background = background_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_background = background_filter_model.linear1.bias.data[0].numpy()\n",
    "\n",
    "\n",
    "class KirigamiExtractor(AudioFeatureExtractor):\n",
    "    def apply_kirigami_filter(self, stft, weight, bias, threshold=0.3):\n",
    "        \"\"\"Apply Kirigami logistic regression filter on STFT features (ensuring 129 features per frame).\"\"\"\n",
    "        output_stft = np.zeros_like(stft)\n",
    "\n",
    "        for i, frame in enumerate(stft):\n",
    "            frame = frame[:129]  # Ensure exactly 129 dimensions\n",
    "\n",
    "            sum_val = np.sum(frame) + 1e-6  # Avoid division by zero\n",
    "            product = np.dot(frame, weight) / sum_val + bias\n",
    "            prob = 1 / (1 + np.exp(-product))  # Sigmoid activation\n",
    "\n",
    "            if prob < threshold:  # If probability is low, keep frame\n",
    "                output_stft[i, :129] = frame  # Apply only to the valid region\n",
    "\n",
    "        return output_stft\n",
    "\n",
    "    # Process Audio File\n",
    "    def process_audio(self, audio, sr=16000, threshold=0.5):\n",
    "        \"\"\"Process an audio file through Kirigami models and save the filtered output.\"\"\"\n",
    "        # Load audio\n",
    "        # audio, sr = librosa.load(input_audio_path, sr=16000)\n",
    "\n",
    "        # Compute STFT (Ensure output has 129 feature bins)\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=256, hop_length=128))[:129, :].T  # Transpose for correct shape\n",
    "\n",
    "        # Apply Kirigami phoneme & background filters\n",
    "        filtered_stft_phoneme = self.apply_kirigami_filter(stft, weight_phoneme, bias_phoneme, threshold)\n",
    "        # filtered_stft_background = self.apply_kirigami_filter(filtered_stft_phoneme, weight_background, bias_background, threshold)\n",
    "\n",
    "        # Convert back to audio using inverse STFT\n",
    "        filtered_audio = librosa.istft(filtered_stft_phoneme.T, hop_length=128)\n",
    "\n",
    "        return filtered_audio\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class RawAudioExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        return audio\n",
    "\n",
    "class SyntheticSensorExtractor(AudioFeatureExtractor):\n",
    "    def __init__(self, window_length=256, hop_length=128, samples_per_window=10):\n",
    "        super().__init__()\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "        self.samples_per_window = samples_per_window\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        frames = [\n",
    "            audio[i:i + self.window_length]\n",
    "            for i in range(0, len(audio) - self.window_length + 1, self.hop_length)\n",
    "        ]\n",
    "        \n",
    "        processed_audio = np.zeros(len(audio))\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            fft = np.fft.fft(frame, n=self.window_length)\n",
    "            bins = np.array_split(fft, self.samples_per_window)\n",
    "            reduced_fft = np.array([np.mean(bin) for bin in bins])\n",
    "            \n",
    "            reconstructed_fft = np.zeros_like(fft, dtype=np.complex_)\n",
    "            step = len(fft) // len(reduced_fft)\n",
    "            \n",
    "            for j, val in enumerate(reduced_fft):\n",
    "                reconstructed_fft[j * step:(j + 1) * step] = val\n",
    "            \n",
    "            processed_frame = np.fft.ifft(reconstructed_fft).real\n",
    "            start = i * self.hop_length\n",
    "            end = min(start + self.window_length, len(processed_audio))\n",
    "            processed_audio[start:end] += processed_frame[:end - start]\n",
    "        \n",
    "        if np.max(np.abs(processed_audio)) > 0:\n",
    "            processed_audio = processed_audio / np.max(np.abs(processed_audio))\n",
    "            \n",
    "        return processed_audio\n",
    "\n",
    "class PrivacyMicExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        S = librosa.stft(audio, n_fft=256, hop_length=128)\n",
    "        frequencies = librosa.fft_frequencies(sr=self.sr, n_fft=256)\n",
    "        S_filtered = np.where(frequencies[:, None] <= 300, S, 0)\n",
    "        return librosa.istft(S_filtered, hop_length=128)\n",
    "\n",
    "class CoughSenseExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        n_fft = int(0.15 * self.sr)\n",
    "        if n_fft % 2 != 0:\n",
    "            n_fft += 1\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        S = np.abs(librosa.stft(audio.astype(np.float64), n_fft=n_fft, hop_length=hop_length, window='hamming'))\n",
    "        S_flattened = S.T\n",
    "        \n",
    "        pca = PCA(n_components=10)\n",
    "        S_reduced = pca.fit_transform(S_flattened)\n",
    "        S_reconstructed = pca.inverse_transform(S_reduced).T\n",
    "\n",
    "        filtered_audio = librosa.istft(S_reconstructed, \n",
    "                                     hop_length=hop_length,\n",
    "                                     win_length=n_fft,\n",
    "                                     window='hamming')\n",
    "        return filtered_audio\n",
    "\n",
    "class SamosaExtractor(AudioFeatureExtractor):\n",
    "    def __init__(self, window_length=0.6, hop_length=0.03):\n",
    "        super().__init__()\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        subsampled_audio = signal.resample(audio, int(len(audio) * 1000 / self.sr))\n",
    "        upsampled_audio = signal.resample(subsampled_audio, len(audio))\n",
    "        return upsampled_audio\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, feature_extractor: AudioFeatureExtractor, ds_path: str, extractor_name: str):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.ds_path = ds_path\n",
    "        self.extractor_name = extractor_name\n",
    "        self.features_path = \"extracted_features\"\n",
    "        os.makedirs(self.features_path, exist_ok=True)\n",
    "\n",
    "    def create_features_df(self, data):\n",
    "        features_filepath = os.path.join(self.features_path, f\"{self.extractor_name}_features2000.csv\")\n",
    "        features_list = []\n",
    "        \n",
    "        for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "            features = self.feature_extractor.extract_features(\n",
    "                os.path.join(self.ds_path, row['filename'])\n",
    "            )\n",
    "           \n",
    "            if features is not None:\n",
    "                for i in features:\n",
    "                    features_list.append([row['emotion']] + i.tolist())\n",
    "\n",
    "        feature_columns = [f\"feature_{i}\" for i in range(len(features_list[0])-1)]\n",
    "        features_df = pd.DataFrame(features_list, columns=['emotion'] + feature_columns)\n",
    "        features_df.to_csv(features_filepath, index=False)\n",
    "        return features_df\n",
    "\n",
    "    def run_classification(self, features_df, target_col, classes, title):\n",
    "        \n",
    "        df_filtered = features_df[features_df[target_col].isin(classes)]\n",
    "        \n",
    "        # Drop rows where all feature values are zero\n",
    "        df_filtered = df_filtered.loc[~(df_filtered.drop([target_col], axis=1) == 0).all(axis=1)]\n",
    "        \n",
    "        X = df_filtered.drop([target_col], axis=1)\n",
    "        y = df_filtered[target_col]\n",
    "        \n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        print(f\"\\nAccuracy for {title}: {accuracy:.4f}\")\n",
    "        print(f\"\\nClassification Report for {title}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=classes))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "def print_accuracy_summary(accuracies):\n",
    "    print(\"\\n=== ACCURACY SUMMARY ===\")\n",
    "    \n",
    "    for extractor, score in accuracies.items():\n",
    "        print(f\"\\n{extractor.capitalize()}:\")\n",
    "        print(f\"  Emotion: {score:.2%}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    ds_path = \"/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/kirigami/kirigami_filters/CREMA_DATASET\"  # Update with your CREMA-D audio directory\n",
    "\n",
    "    # Create metadata for CREMA-D using the provided snippet\n",
    "    crema_directory_list = os.listdir(ds_path)\n",
    "\n",
    "    file_emotion = []\n",
    "    file_path = []\n",
    "\n",
    "    for file in crema_directory_list:\n",
    "        if file.endswith(\".wav\"):  # Ensure only WAV files are processed\n",
    "            # storing file paths\n",
    "            file_path.append(os.path.join(ds_path, file))\n",
    "            # storing file emotions\n",
    "            part = file.split('_')\n",
    "            if part[2] == 'SAD':\n",
    "                file_emotion.append('sad')\n",
    "            elif part[2] == 'ANG':\n",
    "                file_emotion.append('angry')\n",
    "            elif part[2] == 'DIS':\n",
    "                file_emotion.append('disgust')\n",
    "            elif part[2] == 'FEA':\n",
    "                file_emotion.append('fear')\n",
    "            elif part[2] == 'HAP':\n",
    "                file_emotion.append('happy')\n",
    "            elif part[2] == 'NEU':\n",
    "                file_emotion.append('neutral')\n",
    "            else:\n",
    "                file_emotion.append('Unknown')\n",
    "\n",
    "    # dataframe for emotion of files\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "    # dataframe for path of files\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    data = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "    # Filter for happy and sad emotions\n",
    "    data = data[\n",
    "        (data['Emotions'].isin(['happy', 'sad']))\n",
    "    ]\n",
    "\n",
    "    # Rename columns to match expected 'filename' and 'emotion'\n",
    "    data = data.rename(columns={'Path': 'filename', 'Emotions': 'emotion'})\n",
    "\n",
    "    # Select up to 2,000 files, as in the original code\n",
    "    data = data[['filename', 'emotion']].dropna().head(2000)\n",
    "    data['filename'] = data['filename'].apply(os.path.basename)\n",
    "\n",
    "    extractors = {\n",
    "        'kirigami': KirigamiExtractor(),\n",
    "        'coughsense': CoughSenseExtractor(),\n",
    "        'raw': RawAudioExtractor(),\n",
    "        'synthetic': SyntheticSensorExtractor(),\n",
    "        'privacy': PrivacyMicExtractor(),\n",
    "        'samosa': SamosaExtractor()\n",
    "    }\n",
    "\n",
    "    accuracies = {}\n",
    "    for name, extractor in extractors.items():\n",
    "        print(f\"\\nProcessing with {name} extractor...\")\n",
    "        classifier = AudioClassifier(extractor, ds_path, name)\n",
    "        features_df = classifier.create_features_df(data)\n",
    "        \n",
    "        accuracies[name] = classifier.run_classification(\n",
    "            features_df, 'emotion', ['happy', 'sad'], \n",
    "            f\"{name} - Emotion\"\n",
    "        )\n",
    "    \n",
    "    print_accuracy_summary(accuracies)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae431a64-cc01-48bb-9a96-434e20d76f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7636d9c0-8f5e-49a1-9721-ebe5e97b73d4",
   "metadata": {},
   "source": [
    "<h1> Multi -Class Classification-Angry, sad,happy </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a171bc9e-c3dd-440e-90f6-b40382e1ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total items in directory: 7442\n",
      "Sample items (first 5): ['1001_DFA_ANG_XX.wav', '1001_DFA_DIS_XX.wav', '1001_DFA_FEA_XX.wav', '1001_DFA_HAP_XX.wav', '1001_DFA_NEU_XX.wav']\n",
      "\n",
      "Number of WAV files processed: 7442\n",
      "Sample WAV files (first 5): ['/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/CREMAD/AudioWAV/1001_DFA_ANG_XX.wav', '/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/CREMAD/AudioWAV/1001_DFA_DIS_XX.wav', '/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/CREMAD/AudioWAV/1001_DFA_FEA_XX.wav', '/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/CREMAD/AudioWAV/1001_DFA_HAP_XX.wav', '/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/CREMAD/AudioWAV/1001_DFA_NEU_XX.wav']\n",
      "\n",
      "Before any filtering:\n",
      "DataFrame shape: (7442, 2)\n",
      "Unique emotions: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad']\n",
      "Emotion counts:\n",
      " Emotions\n",
      "angry      1271\n",
      "disgust    1271\n",
      "fear       1271\n",
      "happy      1271\n",
      "sad        1271\n",
      "neutral    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final DataFrame before feature extraction:\n",
      "DataFrame shape: (7442, 2)\n",
      "Unique emotions: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad']\n",
      "Emotion counts:\n",
      " emotion\n",
      "angry      1271\n",
      "disgust    1271\n",
      "fear       1271\n",
      "happy      1271\n",
      "sad        1271\n",
      "neutral    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing with kirigami extractor...\n",
      "Processing 3813 files for emotions: ['angry' 'happy' 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3813/3813 [02:59<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for kirigami - Emotion: 0.6463\n",
      "\n",
      "Classification Report for kirigami - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.72      0.60      0.65      2258\n",
      "       angry       0.61      0.54      0.58      2297\n",
      "         sad       0.62      0.80      0.70      2202\n",
      "\n",
      "    accuracy                           0.65      6757\n",
      "   macro avg       0.65      0.65      0.64      6757\n",
      "weighted avg       0.65      0.65      0.64      6757\n",
      "\n",
      "\n",
      "Processing with coughsense extractor...\n",
      "Processing 3813 files for emotions: ['angry' 'happy' 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 3170/3813 [10:32<01:51,  5.77it/s]/home/sgomasta_umass_edu/.local/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:789: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n",
      "100%|██████████| 3813/3813 [12:49<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for coughsense - Emotion: 0.7485\n",
      "\n",
      "Classification Report for coughsense - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.80      0.72      0.76      2297\n",
      "       angry       0.65      0.65      0.65      2190\n",
      "         sad       0.80      0.88      0.84      2210\n",
      "\n",
      "    accuracy                           0.75      6697\n",
      "   macro avg       0.75      0.75      0.75      6697\n",
      "weighted avg       0.75      0.75      0.75      6697\n",
      "\n",
      "\n",
      "Processing with raw extractor...\n",
      "Processing 3813 files for emotions: ['angry' 'happy' 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3813/3813 [02:39<00:00, 23.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for raw - Emotion: 0.6894\n",
      "\n",
      "Classification Report for raw - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.78      0.64      0.70      2271\n",
      "       angry       0.63      0.56      0.59      2233\n",
      "         sad       0.67      0.86      0.76      2283\n",
      "\n",
      "    accuracy                           0.69      6787\n",
      "   macro avg       0.69      0.69      0.68      6787\n",
      "weighted avg       0.69      0.69      0.68      6787\n",
      "\n",
      "\n",
      "Processing with synthetic extractor...\n",
      "Processing 3813 files for emotions: ['angry' 'happy' 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3813/3813 [06:05<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for synthetic - Emotion: 0.6614\n",
      "\n",
      "Classification Report for synthetic - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.67      0.69      0.68      2271\n",
      "       angry       0.55      0.51      0.53      2233\n",
      "         sad       0.76      0.78      0.77      2283\n",
      "\n",
      "    accuracy                           0.66      6787\n",
      "   macro avg       0.66      0.66      0.66      6787\n",
      "weighted avg       0.66      0.66      0.66      6787\n",
      "\n",
      "\n",
      "Processing with privacy extractor...\n",
      "Processing 3813 files for emotions: ['angry' 'happy' 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3813/3813 [02:52<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for privacy - Emotion: 0.5829\n",
      "\n",
      "Classification Report for privacy - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.64      0.54      0.58      2272\n",
      "       angry       0.53      0.40      0.46      2272\n",
      "         sad       0.58      0.82      0.68      2226\n",
      "\n",
      "    accuracy                           0.58      6770\n",
      "   macro avg       0.58      0.58      0.57      6770\n",
      "weighted avg       0.58      0.58      0.57      6770\n",
      "\n",
      "\n",
      "Processing with samosa extractor...\n",
      "Processing 3813 files for emotions: ['angry' 'happy' 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3813/3813 [03:09<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for samosa - Emotion: 0.6016\n",
      "\n",
      "Classification Report for samosa - Emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       happy       0.62      0.59      0.61      2271\n",
      "       angry       0.51      0.42      0.46      2233\n",
      "         sad       0.65      0.79      0.71      2283\n",
      "\n",
      "    accuracy                           0.60      6787\n",
      "   macro avg       0.59      0.60      0.59      6787\n",
      "weighted avg       0.59      0.60      0.59      6787\n",
      "\n",
      "\n",
      "=== ACCURACY SUMMARY ===\n",
      "\n",
      "Kirigami:\n",
      "  Emotion: 64.63%\n",
      "\n",
      "Coughsense:\n",
      "  Emotion: 74.85%\n",
      "\n",
      "Raw:\n",
      "  Emotion: 68.94%\n",
      "\n",
      "Synthetic:\n",
      "  Emotion: 66.14%\n",
      "\n",
      "Privacy:\n",
      "  Emotion: 58.29%\n",
      "\n",
      "Samosa:\n",
      "  Emotion: 60.16%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "    \n",
    "\n",
    "class AudioFeatureExtractor(ABC):\n",
    "    def __init__(self, n_mfcc=20, max_pad_length=1000):\n",
    "        self.sr = None\n",
    "        self.n_mfcc = n_mfcc\n",
    "    \n",
    "    def extract_mfcc(self, processed_audio):\n",
    "        window_size = int(0.5 * self.sr)\n",
    "        step_size = int(0.25 * self.sr)\n",
    "        mfcc_list = []\n",
    "    \n",
    "        for start in range(0, len(processed_audio) - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window = processed_audio[start:end]\n",
    "            \n",
    "            if len(window) != window_size:\n",
    "                continue\n",
    "    \n",
    "            mfcc = librosa.feature.melspectrogram(y=window, sr=self.sr)\n",
    "            mfcc_list.append(mfcc.mean(axis=1))\n",
    "    \n",
    "        return mfcc_list\n",
    "        \n",
    "    @abstractmethod\n",
    "    def process_audio(self, audio):\n",
    "        pass\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path)\n",
    "            self.sr = sr\n",
    "            processed_audio = self.process_audio(audio)\n",
    "            return self.extract_mfcc(processed_audio)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        \n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from init_config import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, feature_dim=129):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(feature_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xx):\n",
    "        return self.sigmoid(self.linear1(torch.nn.functional.normalize(xx, p=1.0, dim=1)))\n",
    "\n",
    "\n",
    "def load_kirigami_model():\n",
    "    # Phoneme Model!\n",
    "    my_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_phoneme_filter_model.load_state_dict(\n",
    "    torch.load(lr_phoneme_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_phoneme_filter_model.eval()\n",
    "\n",
    "    return my_phoneme_filter_model\n",
    "\n",
    "\n",
    "def load_background_filter_model():\n",
    "    # Background Model!\n",
    "    my_background_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    my_background_filter_model.load_state_dict(torch.load(bg_lr_checkpoint_path, map_location=torch.device('cpu')))\n",
    "    my_background_filter_model.eval()\n",
    "\n",
    "    return my_background_filter_model\n",
    "\n",
    "\n",
    "def kirigami_filter_torch(s_full, threshold=0.5):\n",
    "    lr_phoneme_filter_model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    # load the model if in kirigami_filters directory\n",
    "    if os.path.exists(\"./kirigami_filters/scipy_phoneme_filter.ckpt\"):\n",
    "        lr_phoneme_filter_model.load_state_dict(\n",
    "            torch.load(\"./kirigami_filters/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Phoneme filter model not found\")\n",
    "\n",
    "    lr_phoneme_filter_model.eval()\n",
    "    pred = (lr_phoneme_filter_model.forward(torch.Tensor(s_full)) >= threshold).long().numpy()\n",
    "    masked = (1 - pred) * s_full\n",
    "    return masked\n",
    "\n",
    "\n",
    "def kirigami_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "\n",
    "        sum = np.sum(fft)\n",
    "\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def kirigami_filter_reverse_fft(stft, stft_original):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight)):\n",
    "            product = product + vv * weight[iw]\n",
    "        product = product / sum\n",
    "        product = product + bias\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"LR filter probability\", i, z)\n",
    "        if z < LR_THRESHOLD:\n",
    "            # add the value\n",
    "            # output_sp[i] = stft[i]\n",
    "            output_sp[i] = stft_original[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "def background_detection_filter(stft):\n",
    "    output_sp = np.zeros_like(stft)\n",
    "    for i, fft in enumerate(stft):\n",
    "        sum = np.sum(fft)\n",
    "        product = 0\n",
    "        for iw, (vv, ww) in enumerate(zip(fft, weight_background)):\n",
    "            product = product + vv * weight_background[iw]\n",
    "        product = product // sum\n",
    "        product = product + bias_background\n",
    "        z = 1 / (1 + np.exp(-product))\n",
    "        # print(\"Background probability: \", i, z)\n",
    "        if z < BACKGROUND_LR_THRESHOLD:  # lower than threshold not background.\n",
    "            # add the value\n",
    "            output_sp[i] = stft[i]\n",
    "    return output_sp\n",
    "\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Kirigami models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_kirigami_model():\n",
    "    \"\"\"Load the phoneme filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/scipy_phoneme_filter.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_background_filter_model():\n",
    "    \"\"\"Load the background noise filter model.\"\"\"\n",
    "    model = LogisticRegressionClassifier(feature_dim=129)\n",
    "    model.load_state_dict(torch.load(\"kirigami_filters/model_checkpoints/noisy_background_scipy_detector.ckpt\", map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load models\n",
    "phoneme_filter_model = load_kirigami_model()\n",
    "background_filter_model = load_background_filter_model()\n",
    "\n",
    "# Extract model weights\n",
    "weight_phoneme = phoneme_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_phoneme = phoneme_filter_model.linear1.bias.data[0].numpy()\n",
    "weight_background = background_filter_model.linear1.weight.data[0].numpy()\n",
    "bias_background = background_filter_model.linear1.bias.data[0].numpy()\n",
    "\n",
    "\n",
    "class KirigamiExtractor(AudioFeatureExtractor):\n",
    "    def apply_kirigami_filter(self, stft, weight, bias, threshold=0.3):\n",
    "        \"\"\"Apply Kirigami logistic regression filter on STFT features (ensuring 129 features per frame).\"\"\"\n",
    "        output_stft = np.zeros_like(stft)\n",
    "\n",
    "        for i, frame in enumerate(stft):\n",
    "            frame = frame[:129]  # Ensure exactly 129 dimensions\n",
    "\n",
    "            sum_val = np.sum(frame) + 1e-6  # Avoid division by zero\n",
    "            product = np.dot(frame, weight) / sum_val + bias\n",
    "            prob = 1 / (1 + np.exp(-product))  # Sigmoid activation\n",
    "\n",
    "            if prob < threshold:  # If probability is low, keep frame\n",
    "                output_stft[i, :129] = frame  # Apply only to the valid region\n",
    "\n",
    "        return output_stft\n",
    "\n",
    "    # Process Audio File\n",
    "    def process_audio(self, audio, sr=16000, threshold=0.5):\n",
    "        \"\"\"Process an audio file through Kirigami models and save the filtered output.\"\"\"\n",
    "        # Load audio\n",
    "        # audio, sr = librosa.load(input_audio_path, sr=16000)\n",
    "\n",
    "        # Compute STFT (Ensure output has 129 feature bins)\n",
    "        stft = np.abs(librosa.stft(audio, n_fft=256, hop_length=128))[:129, :].T  # Transpose for correct shape\n",
    "\n",
    "        # Apply Kirigami phoneme & background filters\n",
    "        filtered_stft_phoneme = self.apply_kirigami_filter(stft, weight_phoneme, bias_phoneme, threshold)\n",
    "        # filtered_stft_background = self.apply_kirigami_filter(filtered_stft_phoneme, weight_background, bias_background, threshold)\n",
    "\n",
    "        # Convert back to audio using inverse STFT\n",
    "        filtered_audio = librosa.istft(filtered_stft_phoneme.T, hop_length=128)\n",
    "\n",
    "        return filtered_audio\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class RawAudioExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        return audio\n",
    "\n",
    "class SyntheticSensorExtractor(AudioFeatureExtractor):\n",
    "    def __init__(self, window_length=256, hop_length=128, samples_per_window=10):\n",
    "        super().__init__()\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "        self.samples_per_window = samples_per_window\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        frames = [\n",
    "            audio[i:i + self.window_length]\n",
    "            for i in range(0, len(audio) - self.window_length + 1, self.hop_length)\n",
    "        ]\n",
    "        \n",
    "        processed_audio = np.zeros(len(audio))\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            fft = np.fft.fft(frame, n=self.window_length)\n",
    "            bins = np.array_split(fft, self.samples_per_window)\n",
    "            reduced_fft = np.array([np.mean(bin) for bin in bins])\n",
    "            \n",
    "            reconstructed_fft = np.zeros_like(fft, dtype=np.complex_)\n",
    "            step = len(fft) // len(reduced_fft)\n",
    "            \n",
    "            for j, val in enumerate(reduced_fft):\n",
    "                reconstructed_fft[j * step:(j + 1) * step] = val\n",
    "            \n",
    "            processed_frame = np.fft.ifft(reconstructed_fft).real\n",
    "            start = i * self.hop_length\n",
    "            end = min(start + self.window_length, len(processed_audio))\n",
    "            processed_audio[start:end] += processed_frame[:end - start]\n",
    "        \n",
    "        if np.max(np.abs(processed_audio)) > 0:\n",
    "            processed_audio = processed_audio / np.max(np.abs(processed_audio))\n",
    "            \n",
    "        return processed_audio\n",
    "\n",
    "class PrivacyMicExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        S = librosa.stft(audio, n_fft=256, hop_length=128)\n",
    "        frequencies = librosa.fft_frequencies(sr=self.sr, n_fft=256)\n",
    "        S_filtered = np.where(frequencies[:, None] <= 300, S, 0)\n",
    "        return librosa.istft(S_filtered, hop_length=128)\n",
    "\n",
    "class CoughSenseExtractor(AudioFeatureExtractor):\n",
    "    def process_audio(self, audio):\n",
    "        n_fft = int(0.15 * self.sr)\n",
    "        if n_fft % 2 != 0:\n",
    "            n_fft += 1\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        S = np.abs(librosa.stft(audio.astype(np.float64), n_fft=n_fft, hop_length=hop_length, window='hamming'))\n",
    "        S_flattened = S.T\n",
    "        \n",
    "        pca = PCA(n_components=10)\n",
    "        S_reduced = pca.fit_transform(S_flattened)\n",
    "        S_reconstructed = pca.inverse_transform(S_reduced).T\n",
    "\n",
    "        filtered_audio = librosa.istft(S_reconstructed, \n",
    "                                     hop_length=hop_length,\n",
    "                                     win_length=n_fft,\n",
    "                                     window='hamming')\n",
    "        return filtered_audio\n",
    "\n",
    "class SamosaExtractor(AudioFeatureExtractor):\n",
    "    def __init__(self, window_length=0.6, hop_length=0.03):\n",
    "        super().__init__()\n",
    "        self.window_length = window_length\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        subsampled_audio = signal.resample(audio, int(len(audio) * 1000 / self.sr))\n",
    "        upsampled_audio = signal.resample(subsampled_audio, len(audio))\n",
    "        return upsampled_audio\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, feature_extractor: AudioFeatureExtractor, ds_path: str, extractor_name: str):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.ds_path = ds_path\n",
    "        self.extractor_name = extractor_name\n",
    "        self.features_path = \"extracted_features\"\n",
    "        os.makedirs(self.features_path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    def create_features_df(self, data):\n",
    "        features_filepath = os.path.join(self.features_path, f\"{self.extractor_name}_features.csv\")\n",
    "        features_list = []\n",
    "                                         \n",
    "        # Filter for happy, angry, and sad emotions during feature extraction\n",
    "        filtered_data = data[data['emotion'].isin(['happy', 'angry', 'sad'])]\n",
    "        print(f\"Processing {len(filtered_data)} files for emotions: {filtered_data['emotion'].unique()}\")\n",
    "        \n",
    "        for _, row in tqdm(filtered_data.iterrows(), total=len(filtered_data)):\n",
    "            features = self.feature_extractor.extract_features(\n",
    "                os.path.join(self.ds_path, row['filename'])\n",
    "            )\n",
    "           \n",
    "            if features is not None:\n",
    "                for i in features:\n",
    "                    features_list.append([row['emotion']] + i.tolist())\n",
    "                                \n",
    "        if not features_list:\n",
    "            raise ValueError(\"No features were extracted. Check if audio files are accessible or if extract_features is working correctly.\")\n",
    "\n",
    "        feature_columns = [f\"feature_{i}\" for i in range(len(features_list[0])-1)]\n",
    "        features_df = pd.DataFrame(features_list, columns=['emotion'] + feature_columns)\n",
    "        features_df.to_csv(features_filepath, index=False)\n",
    "        return features_df\n",
    "\n",
    "    def run_classification(self, features_df, target_col, classes, title):\n",
    "        \n",
    "        df_filtered = features_df[features_df[target_col].isin(classes)]\n",
    "        \n",
    "        # Drop rows where all feature values are zero\n",
    "        df_filtered = df_filtered.loc[~(df_filtered.drop([target_col], axis=1) == 0).all(axis=1)]\n",
    "        \n",
    "        X = df_filtered.drop([target_col], axis=1)\n",
    "        y = df_filtered[target_col]\n",
    "        \n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        print(f\"\\nAccuracy for {title}: {accuracy:.4f}\")\n",
    "        print(f\"\\nClassification Report for {title}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=classes))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "def print_accuracy_summary(accuracies):\n",
    "    print(\"\\n=== ACCURACY SUMMARY ===\")\n",
    "    \n",
    "    for extractor, score in accuracies.items():\n",
    "        print(f\"\\n{extractor.capitalize()}:\")\n",
    "        print(f\"  Emotion: {score:.2%}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    ds_path = \"/work/pi_shenoy_umass_edu/sgomasta_umass_edu/Batch Files/Privacy Peserving Audio/Common Voice/bhawana/CREMAD/AudioWAV\"  # Updated path\n",
    "\n",
    "    # Create metadata for CREMA-D using the provided snippet\n",
    "    crema_directory_list = os.listdir(ds_path)\n",
    "\n",
    "    print(f\"\\nTotal items in directory: {len(crema_directory_list)}\")\n",
    "    print(\"Sample items (first 5):\", crema_directory_list[:5])\n",
    "\n",
    "    file_emotion = []\n",
    "    file_path = []\n",
    "\n",
    "    for file in crema_directory_list:\n",
    "        if file.lower().endswith(\".wav\"):  # Case-insensitive check\n",
    "            # storing file paths\n",
    "            file_path.append(os.path.join(ds_path, file))\n",
    "            # storing file emotions\n",
    "            part = file.split('_')\n",
    "            if len(part) < 3:\n",
    "                print(f\"Skipping file {file}: Invalid format (less than 3 parts after splitting).\")\n",
    "                file_emotion.append('Unknown')\n",
    "                continue\n",
    "            if part[2] == 'SAD':\n",
    "                file_emotion.append('sad')\n",
    "            elif part[2] == 'ANG':\n",
    "                file_emotion.append('angry')\n",
    "            elif part[2] == 'DIS':\n",
    "                file_emotion.append('disgust')\n",
    "            elif part[2] == 'FEA':\n",
    "                file_emotion.append('fear')\n",
    "            elif part[2] == 'HAP':\n",
    "                file_emotion.append('happy')\n",
    "            elif part[2] == 'NEU':\n",
    "                file_emotion.append('neutral')\n",
    "            else:\n",
    "                file_emotion.append('Unknown')\n",
    "\n",
    "    # Debug: Check the number of WAV files processed\n",
    "    print(f\"\\nNumber of WAV files processed: {len(file_path)}\")\n",
    "    print(\"Sample WAV files (first 5):\", file_path[:5] if file_path else [])\n",
    "\n",
    "    # dataframe for emotion of files\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "    # dataframe for path of files\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    data = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "    # Debug: Print DataFrame info before filtering\n",
    "    print(\"\\nBefore any filtering:\")\n",
    "    print(\"DataFrame shape:\", data.shape)\n",
    "    print(\"Unique emotions:\", data['Emotions'].unique())\n",
    "    print(\"Emotion counts:\\n\", data['Emotions'].value_counts())\n",
    "\n",
    "    # Rename columns to match expected 'filename' and 'emotion'\n",
    "    data = data.rename(columns={'Path': 'filename', 'Emotions': 'emotion'})\n",
    "\n",
    "    # Process the entire dataset (no filtering for emotions)\n",
    "    data = data[['filename', 'emotion']].dropna()\n",
    "    data['filename'] = data['filename'].apply(os.path.basename)\n",
    "\n",
    "    # Debug: Final DataFrame info\n",
    "    print(\"\\nFinal DataFrame before feature extraction:\")\n",
    "    print(\"DataFrame shape:\", data.shape)\n",
    "    print(\"Unique emotions:\", data['emotion'].unique())\n",
    "    print(\"Emotion counts:\\n\", data['emotion'].value_counts())\n",
    "\n",
    "    extractors = {\n",
    "        'kirigami': KirigamiExtractor(),\n",
    "        'coughsense': CoughSenseExtractor(),\n",
    "        'raw': RawAudioExtractor(),\n",
    "        'synthetic': SyntheticSensorExtractor(),\n",
    "        'privacy': PrivacyMicExtractor(),\n",
    "        'samosa': SamosaExtractor()\n",
    "    }\n",
    "\n",
    "    accuracies = {}\n",
    "    for name, extractor in extractors.items():\n",
    "        print(f\"\\nProcessing with {name} extractor...\")\n",
    "        classifier = AudioClassifier(extractor, ds_path, name)\n",
    "        features_df = classifier.create_features_df(data)\n",
    "        \n",
    "        accuracies[name] = classifier.run_classification(\n",
    "            features_df, 'emotion', ['happy', 'angry', 'sad'], \n",
    "            f\"{name} - Emotion\"\n",
    "        )\n",
    "    \n",
    "    print_accuracy_summary(accuracies)\n",
    "                                         \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b2500-acb6-4b5d-8832-db56caf47d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
